{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Detectron2 SageMaker Demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.pytorch import estimator, PyTorchModel, PyTorchPredictor, PyTorch\n",
    "\n",
    "# get our execution role giving us permissions to do things like launch training jobs\n",
    "role = get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "sess = sagemaker.Session() # can use LocalSession() to run container locally\n",
    "\n",
    "bucket = 'privisaa-bucket-2' # sess.default_bucket()\n",
    "region = \"us-east-1\"\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "prefix_input = 'detectron2-input'\n",
    "prefix_output = 'detectron2-output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install SageMaker Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-experiments==0.1.24 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (0.1.24)\n",
      "Requirement already satisfied: boto3>=1.12.8 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker-experiments==0.1.24) (1.15.8)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments==0.1.24) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments==0.1.24) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.8 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments==0.1.24) (1.18.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.19.0,>=1.18.8->boto3>=1.12.8->sagemaker-experiments==0.1.24) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.19.0,>=1.18.8->boto3>=1.12.8->sagemaker-experiments==0.1.24) (1.25.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.8->boto3>=1.12.8->sagemaker-experiments==0.1.24) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install sagemaker-experiments==0.1.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data for training\n",
    "\n",
    "We are grabbing data from COCO, decompressing the data, and then sending it to s3. In this notebook we have two examples, one using s3 and one using EFS, if you want to utilize the EFS example, you'll need to mount your EFS volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create stage directory: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32\n",
      "--2020-10-15 00:24:32--  http://images.cocodataset.org/zips/train2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.44.140\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.44.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19336861798 (18G) [application/zip]\n",
      "Saving to: ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/train2017.zip’\n",
      "\n",
      "/home/ec2-user/Sage 100%[===================>]  18.01G  74.1MB/s    in 3m 59s  \n",
      "\n",
      "2020-10-15 00:28:32 (77.0 MB/s) - ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/train2017.zip’ saved [19336861798/19336861798]\n",
      "\n",
      "Extracting /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/train2017.zip\n",
      "============================================================================================================================================================================================================================================Done.\n",
      "--2020-10-15 00:30:14--  http://images.cocodataset.org/zips/val2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.33.163\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.33.163|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 815585330 (778M) [application/zip]\n",
      "Saving to: ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/val2017.zip’\n",
      "\n",
      "/home/ec2-user/Sage 100%[===================>] 777.80M  95.0MB/s    in 8.1s    \n",
      "\n",
      "2020-10-15 00:30:22 (95.7 MB/s) - ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/val2017.zip’ saved [815585330/815585330]\n",
      "\n",
      "Extracting /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/val2017.zip\n",
      "==========Done.\n",
      "--2020-10-15 00:30:27--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.249.12\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.249.12|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252907541 (241M) [application/zip]\n",
      "Saving to: ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations_trainval2017.zip’\n",
      "\n",
      "/home/ec2-user/Sage 100%[===================>] 241.19M  96.0MB/s    in 2.5s    \n",
      "\n",
      "2020-10-15 00:30:30 (96.0 MB/s) - ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations_trainval2017.zip’ saved [252907541/252907541]\n",
      "\n",
      "Archive:  /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations_trainval2017.zip\n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/instances_train2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/instances_val2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/captions_train2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/captions_val2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/person_keypoints_train2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/person_keypoints_val2017.json  \n",
      "Thu Oct 15 00:30:36 UTC 2020: Uploading extracted files to s3://privisaa-bucket-2/coco2017 [ eta 12 minutes ]\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================Done.\n",
      "Delete stage directory: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32\n",
      "Success.\n"
     ]
    }
   ],
   "source": [
    "! ./upload_coco2017_to_s3.sh {bucket} coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using EFS, run the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Docker image to registry\n",
    "\n",
    "For this training, we'll extend the [Sagemaker PyTorch Container](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html) with Detectron2 dependencies (using official [D2 Dockerfile](https://github.com/facebookresearch/detectron2/blob/master/docker/Dockerfile)) as baseline. See Dockerfile below.\n",
    "\n",
    "You are in no means limited to using our containers, for examples of jobs using outside containers see:\n",
    "\n",
    "[SageMaker Nvidia NGC Examples ](https://github.com/aws-samples/amazon-sagemaker-nvidia-ngc-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Build an image of Detectron2 that can do distributing training on Amazon Sagemaker \u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# using Sagemaker PyTorch container as base image\u001b[39;49;00m\n",
      "\u001b[37m# https://github.com/aws/sagemaker-pytorch-container/blob/master/docker/1.4.0/py3/Dockerfile.gpu\u001b[39;49;00m\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.1-gpu-py36-cu101-ubuntu16.04\u001b[39;49;00m\n",
      "\u001b[34mLABEL\u001b[39;49;00m \u001b[31mauthor\u001b[39;49;00m=\u001b[33m\"vadimd@amazon.com\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############# Installing latest builds ############\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# This is to fix issue: https://github.com/pytorch/vision/issues/1489\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install --upgrade --force-reinstall \u001b[31mtorch\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.5.1 \u001b[31mtorchvision\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.6.1 cython\n",
      "\u001b[37m# RUN pip install torchvision==0.7.0\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############# D2 section ##############\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# installing dependecies for D2 https://github.com/facebookresearch/detectron2/blob/master/docker/Dockerfile\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install \u001b[33m'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install \u001b[33m'git+https://github.com/facebookresearch/fvcore'\u001b[39;49;00m \n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mFORCE_CUDA\u001b[39;49;00m=\u001b[33m\"1\"\u001b[39;49;00m\n",
      "\u001b[37m# Build D2 only for Volta architecture - V100 chips (ml.p3 AWS instances)\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mTORCH_CUDA_ARCH_LIST\u001b[39;49;00m=\u001b[33m\"Volta\"\u001b[39;49;00m \n",
      "\n",
      "\u001b[37m# Build D2 from latest sources\u001b[39;49;00m\n",
      "\u001b[37m# RUN pip install 'git+https://github.com/zhanghang1989/detectron2-ResNeSt.git'\u001b[39;49;00m\n",
      "\u001b[37m# https://github.com/zhanghang1989/detectron2-ResNeSt.git\u001b[39;49;00m\n",
      "\u001b[37m# https://github.com/facebookresearch/detectron2.git \u001b[39;49;00m\n",
      "\u001b[37m# latest version requires torch.jit.is_tracing which is only in torch 1.6.0\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Set a fixed model cache directory. Detectron2 requirement\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mFVCORE_CACHE\u001b[39;49;00m=\u001b[33m\"/tmp\"\u001b[39;49;00m\n",
      "\u001b[37m# set location of training datasetm, Sagemaker containers copy all data from S3 to /opt/ml/input/data/{channels}\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mDETECTRON2_DATASETS\u001b[39;49;00m=\u001b[33m\"/opt/ml/input/data/training\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############# SageMaker section ##############\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mCOPY\u001b[39;49;00m container_training /opt/ml/code\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /opt/ml/code\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# cloning D2 to code dir as we need access to default congigs\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m git clone \u001b[33m'https://github.com/facebookresearch/detectron2.git'\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m \u001b[36mcd\u001b[39;49;00m detectron2 && git checkout be792b9 \n",
      "\u001b[34mRUN\u001b[39;49;00m python -m pip install -e detectron2\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_PROGRAM train_coco.py\n",
      "\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Starts PyTorch distributed framework\u001b[39;49;00m\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m [\u001b[33m\"bash\"\u001b[39;49;00m, \u001b[33m\"-m\"\u001b[39;49;00m, \u001b[33m\"start_with_right_hostname.sh\"\u001b[39;49;00m]\n"
     ]
    }
   ],
   "source": [
    "!pygmentize Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to build your container from this Dockerfile and push it to Amazon Elastic Container Registry using the `build_and_push.sh` script. But you'll need to loging to Sagemaker ECR and your private ECR first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "# loging to Sagemaker ECR with Deep Learning Containers\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "# loging to your private ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 553020858742.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can push your D2 container to Amazon Elastic Container Registry (ECR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  511.6MB\n",
      "Step 1/18 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.1-gpu-py36-cu101-ubuntu16.04\n",
      " ---> 2aa16a1b866d\n",
      "Step 2/18 : LABEL author=\"vadimd@amazon.com\"\n",
      " ---> Using cache\n",
      " ---> 5e2766146cbe\n",
      "Step 3/18 : RUN pip install --upgrade --force-reinstall torch==1.5.1 torchvision==0.6.1 cython\n",
      " ---> Using cache\n",
      " ---> 24840acf45f9\n",
      "Step 4/18 : RUN pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
      " ---> Using cache\n",
      " ---> 07b33cfeee9a\n",
      "Step 5/18 : RUN pip install 'git+https://github.com/facebookresearch/fvcore'\n",
      " ---> Using cache\n",
      " ---> 17df2af93165\n",
      "Step 6/18 : ENV FORCE_CUDA=\"1\"\n",
      " ---> Using cache\n",
      " ---> 3d6698291fd7\n",
      "Step 7/18 : ENV TORCH_CUDA_ARCH_LIST=\"Volta\"\n",
      " ---> Using cache\n",
      " ---> d78460d1df58\n",
      "Step 8/18 : ENV FVCORE_CACHE=\"/tmp\"\n",
      " ---> Using cache\n",
      " ---> 646f1ef9d106\n",
      "Step 9/18 : ENV DETECTRON2_DATASETS=\"/opt/ml/input/data/training\"\n",
      " ---> Using cache\n",
      " ---> 3e3f0346658b\n",
      "Step 10/18 : COPY container_training /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 10ca38c68944\n",
      "Step 11/18 : WORKDIR /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 9ec96ce62166\n",
      "Step 12/18 : RUN git clone 'https://github.com/facebookresearch/detectron2.git'\n",
      " ---> Using cache\n",
      " ---> bc7e5827f31b\n",
      "Step 13/18 : RUN cd detectron2 && git checkout be792b9\n",
      " ---> Using cache\n",
      " ---> 2cd2ccd88e25\n",
      "Step 14/18 : RUN python -m pip install -e detectron2\n",
      " ---> Using cache\n",
      " ---> 3355742b6348\n",
      "Step 15/18 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 4fc2784273aa\n",
      "Step 16/18 : ENV SAGEMAKER_PROGRAM train_coco.py\n",
      " ---> Using cache\n",
      " ---> 1efea05db2b6\n",
      "Step 17/18 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 634c877f9121\n",
      "Step 18/18 : ENTRYPOINT [\"bash\", \"-m\", \"start_with_right_hostname.sh\"]\n",
      " ---> Using cache\n",
      " ---> 7b52866133e4\n",
      "Successfully built 7b52866133e4\n",
      "Successfully tagged d2-sm-coco:latest\n",
      "The push refers to repository [209419068016.dkr.ecr.us-east-1.amazonaws.com/d2-sm-coco]\n",
      "\n",
      "\u001b[1B7e6e230c: Preparing \n",
      "\u001b[1Bd3858370: Preparing \n",
      "\u001b[1B5a3b652d: Preparing \n",
      "\u001b[1Bd774e78d: Preparing \n",
      "\u001b[1B4e3e6893: Preparing \n",
      "\u001b[1B31a3265c: Preparing \n",
      "\u001b[1B64232b07: Preparing \n",
      "\u001b[1Bd1449436: Preparing \n",
      "\u001b[1B489ec215: Preparing \n",
      "\u001b[1B211bfa77: Preparing \n",
      "\u001b[1Bb2eeb3a9: Preparing \n",
      "\u001b[1Ba0755a7c: Preparing \n",
      "\u001b[1B20d289c4: Preparing \n",
      "\u001b[1Beb6d9aa4: Preparing \n",
      "\u001b[1B826f51f5: Preparing \n",
      "\u001b[1Bf0501057: Preparing \n",
      "\u001b[1B1bafa536: Preparing \n",
      "\u001b[1Be87dd341: Preparing \n",
      "\u001b[1B80613f50: Preparing \n",
      "\u001b[14B4232b07: Waiting g \n",
      "\u001b[14B1449436: Waiting g \n",
      "\u001b[1B27d48048: Preparing \n",
      "\u001b[1B0c7dc417: Preparing \n",
      "\u001b[14B2eeb3a9: Waiting g \n",
      "\u001b[16B11bfa77: Waiting g \n",
      "\u001b[1B48c6969a: Preparing \n",
      "\u001b[1B7f090df2: Preparing \n",
      "\u001b[14B26f51f5: Waiting g \n",
      "\u001b[1Bdadd4466: Preparing \n",
      "\u001b[9B27d48048: Waiting g \n",
      "\u001b[11Baceafa5: Waiting g \n",
      "\u001b[13B97b3058: Waiting g \n",
      "\u001b[1B9683cb41: Preparing \n",
      "\u001b[17B87dd341: Waiting g \n",
      "\u001b[10B8c6969a: Waiting g \n",
      "\u001b[9B20da503d: Waiting g \n",
      "\u001b[2Be637fbff: Layer already exists \u001b[31A\u001b[2K\u001b[28A\u001b[2K\u001b[23A\u001b[2K\u001b[18A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[3A\u001b[2Kdistributed: digest: sha256:68539261d4c7bf0ea10fa68e84a116d632d874919cc98135eb10c53883580af0 size: 8100\n"
     ]
    }
   ],
   "source": [
    "! ./build_and_push.sh d2-sm-coco distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a variation on the main container designed to work with EFS instead of s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some algorithm metrics. SageMaker will scrape the logs from our training job and render them in the training job console. The metrics we are defining are pretty standard, you just need to define the regex to find them, feel free to define your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"d2-sm-coco\" # your container name\n",
    "tag = \"distributed\"\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{container}:{tag}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'total_loss', 'Regex': '.*total_loss:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_cls', 'Regex': '.*loss_cls:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_box_reg', 'Regex': '.*loss_box_reg:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_mask', 'Regex': '.*loss_mask:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_rpn_cls', 'Regex': '.*loss_rpn_cls:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_rpn_loc', 'Regex': '.*loss_rpn_loc:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'overall_training_speed',\n",
       "  'Regex': '.*Overall training speed:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'lr', 'Regex': '.*lr:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'iter', 'Regex': '.*iter:\\\\s([0-9\\\\.]+)\\\\s*'}]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_definitions=[{\n",
    "        \"Name\": \"total_loss\",\n",
    "        \"Regex\": \".*total_loss:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_cls\",\n",
    "        \"Regex\": \".*loss_cls:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_box_reg\",\n",
    "        \"Regex\": \".*loss_box_reg:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_mask\",\n",
    "        \"Regex\": \".*loss_mask:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_rpn_cls\",\n",
    "        \"Regex\": \".*loss_rpn_cls:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_rpn_loc\",\n",
    "        \"Regex\": \".*loss_rpn_loc:\\s([0-9\\\\.]+)\\s*\"\n",
    "    }, \n",
    "    {\n",
    "        \"Name\": \"overall_training_speed\",\n",
    "        \"Regex\": \".*Overall training speed:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"lr\",  \n",
    "        \"Regex\": \".*lr:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"iter\",  \n",
    "        \"Regex\": \".*iter:\\s([0-9\\\\.]+)\\s*\"\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_path = 'metric_defs'\n",
    "\n",
    "with open(metric_path, 'w') as f:\n",
    "    for met in metric_definitions:\n",
    "        f.write(json.dumps(met))\n",
    "        f.write('\\n')\n",
    "        \n",
    "metric_definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Experiments\n",
    "\n",
    "SageMaker experiments needs some setup before we can hook it into our estimators. We first are going to create our tracker and within our tracker, create an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7f3f88ed37b8>,experiment_name='d2-coco-demo-1603299483',description='Detectron2 training on COCO2017',tags=None,experiment_arn='arn:aws:sagemaker:us-east-1:209419068016:experiment/d2-coco-demo-1603299483',response_metadata={'RequestId': '9f26cc0d-76ce-4af3-9220-c6b2300c174e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '9f26cc0d-76ce-4af3-9220-c6b2300c174e', 'content-type': 'application/x-amz-json-1.1', 'content-length': '95', 'date': 'Wed, 21 Oct 2020 16:58:03 GMT'}, 'RetryAttempts': 0})\n"
     ]
    }
   ],
   "source": [
    "# create d2 experiment\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "b3sess = boto3.Session()\n",
    "sm = b3sess.client('sagemaker')\n",
    "\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        \"normalization_mean\": 0.1307,\n",
    "        \"normalization_std\": 0.3081,\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "#     tracker.log_input(name=\"d2-dataset\", media_type=\"s3/uri\", value=inputs)\n",
    "\n",
    "d2_experiment = Experiment.create(\n",
    "    experiment_name=f\"d2-coco-demo-{int(time.time())}\", \n",
    "    description=\"Detectron2 training on COCO2017\", \n",
    "    sagemaker_boto_client=sm)\n",
    "print(d2_experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a trial within our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channel_trial_name_map = {}\n",
    "preprocessing_trial_component = tracker.trial_component\n",
    "\n",
    "trial_name = f\"d2-demo-training-job-{int(time.time())}\"\n",
    "d2_trial = Trial.create(\n",
    "    trial_name=trial_name, \n",
    "    experiment_name=d2_experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "hidden_channel_trial_name_map[0] = trial_name\n",
    "\n",
    "# associate the proprocessing trial component with the current trial\n",
    "d2_trial.add_trial_component(preprocessing_trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Hyperparameters\n",
    "\n",
    "Let's set our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/detectron2/ImageNetPretrained/MSRA/R-101.pkl .\n",
    "# !aws s3 cp R-101.pkl s3://privisaa-bucket-2/models/mask_rcnn_R_101_C4_3x/R-101.pkl\n",
    "\n",
    "hyperparameters = {\"config-file\":\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\", \n",
    "                   #\"local-config-file\" : \"config.yaml\", # if you'd like to supply custom config file, please add it in container_training folder, and provide file name here\n",
    "                   \"resume\":\"True\", # whether to re-use weights from pre-trained model\n",
    "                   \"eval-only\":\"False\", # whether to perform only D2 model evaluation\n",
    "                  # opts are D2 model configuration as defined here: https://detectron2.readthedocs.io/modules/config.html#config-references\n",
    "                  # this is a way to override individual parameters in D2 configuration from Sagemaker API\n",
    "                   \"opts\": \"SOLVER.MAX_ITER 1000\",\n",
    "                   \"spot_ckpt\":''\n",
    "                   }\n",
    "\n",
    "with open('hyperparams.json', 'w') as f:\n",
    "    json.dump(hyperparameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch a Job via Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating training-job with name: 2-nodes-max-iter-1000-demo2-p3dn\n"
     ]
    }
   ],
   "source": [
    "# sessLocal = sagemaker.LocalSession() # can use LocalSession()\n",
    "    \n",
    "d2 = PyTorch(      image_name = image,\n",
    "                   role=role,\n",
    "                   entry_point='/home/ec2-user/SageMaker/detectron2-sagemaker/container_training/train_coco.py',\n",
    "                   train_instance_count=2, \n",
    "                   train_instance_type= 'ml.p3dn.24xlarge',\n",
    "#                     train_instance_type=\"local_gpu\", # use local_gpu for quick troubleshooting\n",
    "                   train_volume_size=100,\n",
    "                   framework_version='1.5.1',\n",
    "                   source_dir='/home/ec2-user/SageMaker/detectron2-sagemaker/',\n",
    "                   output_path=f\"s3://{bucket}/{prefix_output}\",\n",
    "                   metric_definitions = metric_definitions,\n",
    "                   hyperparameters = hyperparameters, \n",
    "                   sagemaker_session=sess,\n",
    "                   profiler_config=profiler_config)\n",
    "\n",
    "# d2 = sagemaker.estimator.Estimator(image_name=image,\n",
    "#                                    role=role,\n",
    "#                                    train_instance_count=2, \n",
    "#                                    train_instance_type= 'ml.p3.16xlarge',\n",
    "# #                                   train_instance_type=\"local_gpu\", # use local_gpu for quick troubleshooting\n",
    "#                                    train_volume_size=100,\n",
    "#                                    output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "#                                    metric_definitions = metric_definitions,\n",
    "#                                    hyperparameters = hyperparameters, \n",
    "#                                    sagemaker_session=sess,\n",
    "#                                   profiler_config=profiler_config)\n",
    "\n",
    "d2.fit({'training':f\"s3://{bucket}/train-coco\"},\n",
    "       job_name = \"2-nodes-max-iter-1000-demo2-p3dn\",\n",
    "       wait=False,\n",
    "              experiment_config={\n",
    "            \"TrialName\": d2_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch via CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: launch_coco_train_boto3.py run-d2-sm [-h] [--bucket BUCKET]\n",
      "                                            [--image_name IMAGE_NAME]\n",
      "                                            [--metric_path METRIC_PATH]\n",
      "                                            [--job_name JOB_NAME]\n",
      "                                            [--region REGION]\n",
      "                                            [--prefix_input PREFIX_INPUT]\n",
      "                                            [--prefix_output PREFIX_OUTPUT]\n",
      "                                            [--instance_count INSTANCE_COUNT]\n",
      "                                            [--data_prefix DATA_PREFIX]\n",
      "                                            [--instance_type INSTANCE_TYPE]\n",
      "                                            [--volume_size VOLUME_SIZE]\n",
      "                                            [--use_spot] [--role ROLE]\n",
      "                                            [--max_run_time MAX_RUN_TIME]\n",
      "                                            [--max_wait_time MAX_WAIT_TIME]\n",
      "                                            [--d2_config D2_CONFIG]\n",
      "                                            [--hyperparam_path HYPERPARAM_PATH]\n",
      "\n",
      "    Utility for launching detectron2 training jobs using boto3 create_training_job API.\n",
      "    Has options for launching jobs using spot instances, if launching spot,\n",
      "    please set max_wait_time variable. Check different methods for additional documentation\n",
      "    \n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --bucket BUCKET       s3 bucket for data retrieval and storage of results\n",
      "                        (default: 'sagemaker-us-east-1-209419068016')\n",
      "  --image_name IMAGE_NAME\n",
      "                        Name of the Docker image to be used for training\n",
      "                        (default: -)\n",
      "  --metric_path METRIC_PATH\n",
      "                        Location for metric definition file (default: -)\n",
      "  --job_name JOB_NAME   Name of the SageMaker training job to launch (default:\n",
      "                        'd2-coco-train')\n",
      "  --region REGION\n",
      "  --prefix_input PREFIX_INPUT\n",
      "  --prefix_output PREFIX_OUTPUT\n",
      "  --instance_count INSTANCE_COUNT\n",
      "                        Number of instances to train on (default: 2)\n",
      "  --data_prefix DATA_PREFIX\n",
      "                        location in s3 for your data (default: 'train-coco')\n",
      "  --instance_type INSTANCE_TYPE\n",
      "                        Type of EC2 instances to train on, for (default:\n",
      "                        'ml.p3.16xlarge')\n",
      "  --volume_size VOLUME_SIZE\n",
      "                        Size of EBS volume attached to instance (default: 100)\n",
      "  --use_spot            Whether to use spot instances for training (default:\n",
      "                        False)\n",
      "  --role ROLE           SageMaker execution role (default: -)\n",
      "  --max_run_time MAX_RUN_TIME\n",
      "  --max_wait_time MAX_WAIT_TIME\n",
      "  --d2_config D2_CONFIG\n",
      "                        Detectron2 configuration file to use (default: 'COCO-\n",
      "                        InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml')\n",
      "  --hyperparam_path HYPERPARAM_PATH\n",
      "                        Location for hyperparameters file (default: -)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} launch_coco_train_boto3.py run-d2-sm --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job launched!\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} launch_coco_train_boto3.py run-d2-sm --bucket privisaa-bucket-2 --job_name d2-cli-job2 --region us-east-1 --metric_path metric_defs --hyperparam_path hyperparams.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ARN:  arn:aws:sagemaker:us-east-1:209419068016:training-job/d2-cli-job2\n",
      "Job Status:  InProgress\n",
      "Hyperparameters:  {'config-file': 'COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml', 'eval-only': 'False', 'opts': 'SOLVER.MAX_ITER 1000', 'resume': 'True'}\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} launch_coco_train_boto3.py check-d2-sm --job_name d2-cli-job2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Spot Instance with s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use_spot_instances = True\n",
    "train_max_run=21600\n",
    "train_max_wait = 30000 if train_use_spot_instances else None\n",
    "\n",
    "import uuid\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_uri = 's3://{}/artifacts/d2-checkpoint-{}/'.format(bucket, checkpoint_suffix) if train_use_spot_instances else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating training-job with name: 2-nodes-max-iter-2000-genest-prof-spot6\n"
     ]
    }
   ],
   "source": [
    "container = \"d2-sm-coco-custom\" # your container name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/d2-sm-coco:distributed'.format(account, region)\n",
    "\n",
    "hyperparameters = {\"config-file\":\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\", \n",
    "                   #\"local-config-file\" : \"config.yaml\", # if you'd like to supply custom config file, please add it in container_training folder, and provide file name here\n",
    "                   \"resume\":\"True\", # whether to re-use weights from pre-trained model\n",
    "                   \"eval-only\":\"False\", # whether to perform only D2 model evaluation\n",
    "                  # opts are D2 model configuration as defined here: https://detectron2.readthedocs.io/modules/config.html#config-references\n",
    "                  # this is a way to override individual parameters in D2 configuration from Sagemaker API\n",
    "                   \"opts\": \"SOLVER.MAX_ITER 5000 SOLVER.BASE_LR 0.002 SOLVER.CHECKPOINT_PERIOD 1000\",\n",
    "                   \"spot_ckpt\":'s3://privisaa-bucket-2/artifacts/d2-efs-checkpoint-57a35863/model_final.pth'\n",
    "                   }\n",
    "\n",
    "\n",
    "d2 = sagemaker.estimator.Estimator(image,\n",
    "                                   role=role,\n",
    "                                   train_instance_count=2, \n",
    "                                   train_instance_type='ml.p3.2xlarge',\n",
    "                                   train_volume_size=100,\n",
    "                                   output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "                                   metric_definitions = metric_definitions,\n",
    "                                   hyperparameters = hyperparameters, \n",
    "                                   sagemaker_session=sess,\n",
    "                                   train_use_spot_instances=train_use_spot_instances,\n",
    "                                   train_max_run=train_max_run,\n",
    "                                   train_max_wait=train_max_wait,\n",
    "                                   checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "                                   profiler_config=profiler_config\n",
    "                                  )\n",
    "\n",
    "d2.fit({'training':f\"s3://{bucket}/train-coco\"},\n",
    "       job_name = \"2-nodes-max-iter-2000-genest-prof-spot6\",\n",
    "       wait=False,\n",
    "              experiment_config={\n",
    "            \"TrialName\": d2_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>SageMaker.ImageUri</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>config-file</th>\n",
       "      <th>eval-only</th>\n",
       "      <th>opts</th>\n",
       "      <th>resume</th>\n",
       "      <th>sagemaker_container_log_level</th>\n",
       "      <th>sagemaker_enable_cloudwatch_metrics</th>\n",
       "      <th>sagemaker_job_name</th>\n",
       "      <th>sagemaker_program</th>\n",
       "      <th>sagemaker_region</th>\n",
       "      <th>sagemaker_submit_directory</th>\n",
       "      <th>spot_ckpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-nodes-max-iter-1000-ptest-newfpath-resnestno...</td>\n",
       "      <td>Training</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:209419068016:train...</td>\n",
       "      <td>209419068016.dkr.ecr.us-east-1.amazonaws.com/d...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ml.p3.16xlarge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_...</td>\n",
       "      <td>\"False\"</td>\n",
       "      <td>\"SOLVER.MAX_ITER 1000\"</td>\n",
       "      <td>\"True\"</td>\n",
       "      <td>20.0</td>\n",
       "      <td>false</td>\n",
       "      <td>\"2-nodes-max-iter-1000-ptest-newfpath-resnestn...</td>\n",
       "      <td>\"/home/ec2-user/SageMaker/detectron2-sagemaker...</td>\n",
       "      <td>\"us-east-1\"</td>\n",
       "      <td>\"s3://privisaa-bucket-2/2-nodes-max-iter-1000-...</td>\n",
       "      <td>\"\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName DisplayName  \\\n",
       "0  2-nodes-max-iter-1000-ptest-newfpath-resnestno...    Training   \n",
       "\n",
       "                                           SourceArn  \\\n",
       "0  arn:aws:sagemaker:us-east-1:209419068016:train...   \n",
       "\n",
       "                                  SageMaker.ImageUri  SageMaker.InstanceCount  \\\n",
       "0  209419068016.dkr.ecr.us-east-1.amazonaws.com/d...                      2.0   \n",
       "\n",
       "  SageMaker.InstanceType  SageMaker.VolumeSizeInGB  \\\n",
       "0         ml.p3.16xlarge                     100.0   \n",
       "\n",
       "                                         config-file eval-only  \\\n",
       "0  \"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_...   \"False\"   \n",
       "\n",
       "                     opts  resume  sagemaker_container_log_level  \\\n",
       "0  \"SOLVER.MAX_ITER 1000\"  \"True\"                           20.0   \n",
       "\n",
       "  sagemaker_enable_cloudwatch_metrics  \\\n",
       "0                               false   \n",
       "\n",
       "                                  sagemaker_job_name  \\\n",
       "0  \"2-nodes-max-iter-1000-ptest-newfpath-resnestn...   \n",
       "\n",
       "                                   sagemaker_program sagemaker_region  \\\n",
       "0  \"/home/ec2-user/SageMaker/detectron2-sagemaker...      \"us-east-1\"   \n",
       "\n",
       "                          sagemaker_submit_directory spot_ckpt  \n",
       "0  \"s3://privisaa-bucket-2/2-nodes-max-iter-1000-...        \"\"  "
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_expression = {\n",
    "    \"Filters\":[\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=Session(b3sess, sm), \n",
    "    experiment_name=d2_experiment.experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.test:accuracy.max\",\n",
    "    sort_order=\"Descending\",\n",
    "    metric_names=['test:accuracy'],\n",
    ")\n",
    "\n",
    "trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TrainingJobName': 'd2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318',\n",
       " 'TrainingJobArn': 'arn:aws:sagemaker:us-east-1:209419068016:training-job/d2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318',\n",
       " 'ModelArtifacts': {'S3ModelArtifacts': 's3://privisaa-bucket-2/detectron2-output/d2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318/output/model.tar.gz'},\n",
       " 'TrainingJobStatus': 'Completed',\n",
       " 'SecondaryStatus': 'Completed',\n",
       " 'HyperParameters': {'config-file': '\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\"',\n",
       "  'eval-only': '\"False\"',\n",
       "  'opts': '\"SOLVER.MAX_ITER 1000\"',\n",
       "  'resume': '\"True\"',\n",
       "  'sagemaker_container_log_level': '20',\n",
       "  'sagemaker_enable_cloudwatch_metrics': 'false',\n",
       "  'sagemaker_job_name': '\"d2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318\"',\n",
       "  'sagemaker_program': '\"/home/ec2-user/SageMaker/detectron2-sagemaker/container_training/train_coco_efs.py\"',\n",
       "  'sagemaker_region': '\"us-east-1\"',\n",
       "  'sagemaker_submit_directory': '\"s3://privisaa-bucket-2/d2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318/source/sourcedir.tar.gz\"',\n",
       "  'spot_ckpt': '\"\"'},\n",
       " 'AlgorithmSpecification': {'TrainingImage': '209419068016.dkr.ecr.us-east-1.amazonaws.com/d2-sm-coco2:distributed',\n",
       "  'TrainingInputMode': 'File',\n",
       "  'MetricDefinitions': [{'Name': 'total_loss',\n",
       "    'Regex': '.*total_loss:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'loss_cls', 'Regex': '.*loss_cls:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'loss_box_reg', 'Regex': '.*loss_box_reg:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'loss_mask', 'Regex': '.*loss_mask:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'loss_rpn_cls', 'Regex': '.*loss_rpn_cls:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'loss_rpn_loc', 'Regex': '.*loss_rpn_loc:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'overall_training_speed',\n",
       "    'Regex': '.*Overall training speed:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'lr', 'Regex': '.*lr:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'iter', 'Regex': '.*iter:\\\\s([0-9\\\\.]+)\\\\s*'}],\n",
       "  'EnableSageMakerMetricsTimeSeries': True},\n",
       " 'RoleArn': 'arn:aws:iam::209419068016:role/service-role/AmazonSageMaker-ExecutionRole-20200814T102098',\n",
       " 'InputDataConfig': [{'ChannelName': 'train',\n",
       "   'DataSource': {'FileSystemDataSource': {'FileSystemId': 'fs-d120c724',\n",
       "     'FileSystemAccessMode': 'ro',\n",
       "     'FileSystemType': 'EFS',\n",
       "     'DirectoryPath': '/training'}},\n",
       "   'CompressionType': 'None',\n",
       "   'RecordWrapperType': 'None'}],\n",
       " 'OutputDataConfig': {'KmsKeyId': '',\n",
       "  'S3OutputPath': 's3://privisaa-bucket-2/detectron2-output'},\n",
       " 'ResourceConfig': {'InstanceType': 'ml.p3.2xlarge',\n",
       "  'InstanceCount': 2,\n",
       "  'VolumeSizeInGB': 100},\n",
       " 'VpcConfig': {'SecurityGroupIds': ['sg-317ad11a'],\n",
       "  'Subnets': ['subnet-9e445ef9']},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 10000,\n",
       "  'MaxWaitTimeInSeconds': 10000},\n",
       " 'CreationTime': datetime.datetime(2020, 10, 21, 15, 15, 42, 242000, tzinfo=tzlocal()),\n",
       " 'TrainingStartTime': datetime.datetime(2020, 10, 21, 15, 18, 27, 595000, tzinfo=tzlocal()),\n",
       " 'TrainingEndTime': datetime.datetime(2020, 10, 21, 15, 38, 14, 933000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2020, 10, 21, 15, 38, 14, 933000, tzinfo=tzlocal()),\n",
       " 'SecondaryStatusTransitions': [{'Status': 'Starting',\n",
       "   'StartTime': datetime.datetime(2020, 10, 21, 15, 15, 42, 242000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 10, 21, 15, 18, 27, 595000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Preparing the instances for training'},\n",
       "  {'Status': 'Downloading',\n",
       "   'StartTime': datetime.datetime(2020, 10, 21, 15, 18, 27, 595000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 10, 21, 15, 18, 36, 514000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Downloading input data'},\n",
       "  {'Status': 'Training',\n",
       "   'StartTime': datetime.datetime(2020, 10, 21, 15, 18, 36, 514000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 10, 21, 15, 36, 36, 665000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Training image download completed. Training in progress.'},\n",
       "  {'Status': 'Uploading',\n",
       "   'StartTime': datetime.datetime(2020, 10, 21, 15, 36, 36, 665000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 10, 21, 15, 38, 14, 933000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Uploading generated training model'},\n",
       "  {'Status': 'Completed',\n",
       "   'StartTime': datetime.datetime(2020, 10, 21, 15, 38, 14, 933000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 10, 21, 15, 38, 14, 933000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Training job completed'}],\n",
       " 'FinalMetricDataList': [{'MetricName': 'total_loss',\n",
       "   'Value': 1.7690000534057617,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'loss_cls',\n",
       "   'Value': 0.4020000100135803,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'loss_box_reg',\n",
       "   'Value': 0.1979999989271164,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'loss_mask',\n",
       "   'Value': 0.5450000166893005,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'loss_rpn_cls',\n",
       "   'Value': 0.3700000047683716,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'loss_rpn_loc',\n",
       "   'Value': 0.25699999928474426,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'lr',\n",
       "   'Value': 0.01998000033199787,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'iter',\n",
       "   'Value': 1000.0,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())}],\n",
       " 'EnableNetworkIsolation': False,\n",
       " 'EnableInterContainerTrafficEncryption': False,\n",
       " 'EnableManagedSpotTraining': True,\n",
       " 'CheckpointConfig': {'S3Uri': 's3://privisaa-bucket-2/artifacts/d2-efs-checkpoint-57a35863/'},\n",
       " 'TrainingTimeInSeconds': 1187,\n",
       " 'BillableTimeInSeconds': 356,\n",
       " 'DebugHookConfig': {'S3OutputPath': 's3://privisaa-bucket-2/detectron2-output',\n",
       "  'CollectionConfigurations': []},\n",
       " 'ProfilerConfig': {'S3OutputPath': 's3://privisaa-bucket-2/detectron2-ouput',\n",
       "  'ProfilingIntervalInMilliseconds': 500,\n",
       "  'ProfilingParameters': {'GeneralMetricsConfig': '{\"StartStep\": \"2\", \"NumSteps\": \"2\"}',\n",
       "   'ProfilerEnabled': 'True'}},\n",
       " 'ResponseMetadata': {'RequestId': 'c8026794-d5de-4669-b813-06a4a0425e43',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c8026794-d5de-4669-b813-06a4a0425e43',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '4821',\n",
       "   'date': 'Wed, 21 Oct 2020 16:39:50 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.describe_training_job(TrainingJobName='d2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
