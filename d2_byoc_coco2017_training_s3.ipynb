{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Detectron2 SageMaker Demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "import sys\n",
    "import uuid\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.pytorch import estimator, PyTorchModel, PyTorchPredictor, PyTorch\n",
    "\n",
    "# get our execution role giving us permissions to do things like launch training jobs\n",
    "role = get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "sess = sagemaker.Session() # can use LocalSession() to run container locally\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "region = \"us-east-1\"\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "prefix_input = 'detectron2-input'\n",
    "prefix_output = 'detectron2-output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install SageMaker Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-experiments==0.1.24 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (0.1.24)\n",
      "Requirement already satisfied: boto3>=1.12.8 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker-experiments==0.1.24) (1.16.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments==0.1.24) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments==0.1.24) (1.19.1)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments==0.1.24) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.0->boto3>=1.12.8->sagemaker-experiments==0.1.24) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.25.4; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.0->boto3>=1.12.8->sagemaker-experiments==0.1.24) (1.25.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.0->boto3>=1.12.8->sagemaker-experiments==0.1.24) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install sagemaker-experiments==0.1.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data for training\n",
    "\n",
    "We are grabbing data from COCO, decompressing the data, and then sending it to s3. The whole process takes ~15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create stage directory: /home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38\n",
      "--2020-10-22 16:50:38--  http://images.cocodataset.org/zips/train2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.85.188\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.85.188|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19336861798 (18G) [application/zip]\n",
      "Saving to: ‘/home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/train2017.zip’\n",
      "\n",
      "/home/ec2-user/Sage 100%[===================>]  18.01G  68.3MB/s    in 4m 7s   \n",
      "\n",
      "2020-10-22 16:54:44 (74.8 MB/s) - ‘/home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/train2017.zip’ saved [19336861798/19336861798]\n",
      "\n",
      "Extracting /home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/train2017.zip\n",
      "============================================================================================================================================================================================================================================Done.\n",
      "--2020-10-22 16:56:23--  http://images.cocodataset.org/zips/val2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.80.220\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.80.220|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 815585330 (778M) [application/zip]\n",
      "Saving to: ‘/home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/val2017.zip’\n",
      "\n",
      "/home/ec2-user/Sage 100%[===================>] 777.80M  93.0MB/s    in 8.7s    \n",
      "\n",
      "2020-10-22 16:56:32 (89.8 MB/s) - ‘/home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/val2017.zip’ saved [815585330/815585330]\n",
      "\n",
      "Extracting /home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/val2017.zip\n",
      "==========Done.\n",
      "--2020-10-22 16:56:36--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.98.195\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.98.195|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252907541 (241M) [application/zip]\n",
      "Saving to: ‘/home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/annotations_trainval2017.zip’\n",
      "\n",
      "/home/ec2-user/Sage 100%[===================>] 241.19M  96.3MB/s    in 2.5s    \n",
      "\n",
      "2020-10-22 16:56:39 (96.3 MB/s) - ‘/home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/annotations_trainval2017.zip’ saved [252907541/252907541]\n",
      "\n",
      "Archive:  /home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/annotations_trainval2017.zip\n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/annotations/instances_train2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/annotations/instances_val2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/annotations/captions_train2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/annotations/captions_val2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/annotations/person_keypoints_train2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38/annotations/person_keypoints_val2017.json  \n",
      "Thu Oct 22 16:56:45 UTC 2020: Uploading extracted files to s3://sagemaker-us-east-1-209419068016/train-coco/coco [ eta 12 minutes ]\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================Done.\n",
      "Delete stage directory: /home/ec2-user/SageMaker/coco-2017-2020-10-22-16-50-38\n",
      "Success.\n"
     ]
    }
   ],
   "source": [
    "! ./upload_coco2017_to_s3.sh {bucket} train-coco/coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using EFS, run the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Docker image to registry\n",
    "\n",
    "For this training, we'll extend the [Sagemaker PyTorch Container](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html) with Detectron2 dependencies (using official [D2 Dockerfile](https://github.com/facebookresearch/detectron2/blob/master/docker/Dockerfile)) as a baseline. See Dockerfile below.\n",
    "\n",
    "You are in no means limited to using our containers, for examples of jobs using non-SageMaker containers see:\n",
    "\n",
    "[SageMaker Nvidia NGC Examples ](https://github.com/aws-samples/amazon-sagemaker-nvidia-ngc-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Build an image of Detectron2 that can do distributing training on Amazon Sagemaker \u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# using Sagemaker PyTorch container as base image\u001b[39;49;00m\n",
      "\u001b[37m# https://github.com/aws/sagemaker-pytorch-container/blob/master/docker/1.4.0/py3/Dockerfile.gpu\u001b[39;49;00m\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.1-gpu-py36-cu101-ubuntu16.04\u001b[39;49;00m\n",
      "\u001b[34mLABEL\u001b[39;49;00m \u001b[31mauthor\u001b[39;49;00m=\u001b[33m\"vadimd@amazon.com\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############# Installing latest builds ############\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# This is to fix issue: https://github.com/pytorch/vision/issues/1489\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install --upgrade --force-reinstall \u001b[31mtorch\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.5.1 \u001b[31mtorchvision\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.6.1 cython\n",
      "\u001b[37m# RUN pip install torchvision==0.7.0\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############# D2 section ##############\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# installing dependecies for D2 https://github.com/facebookresearch/detectron2/blob/master/docker/Dockerfile\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install \u001b[33m'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install \u001b[33m'git+https://github.com/facebookresearch/fvcore'\u001b[39;49;00m \n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mFORCE_CUDA\u001b[39;49;00m=\u001b[33m\"1\"\u001b[39;49;00m\n",
      "\u001b[37m# Build D2 only for Volta architecture - V100 chips (ml.p3 AWS instances)\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mTORCH_CUDA_ARCH_LIST\u001b[39;49;00m=\u001b[33m\"Volta\"\u001b[39;49;00m \n",
      "\n",
      "\u001b[37m# Build D2 from latest sources\u001b[39;49;00m\n",
      "\u001b[37m# RUN pip install 'git+https://github.com/zhanghang1989/detectron2-ResNeSt.git'\u001b[39;49;00m\n",
      "\u001b[37m# https://github.com/zhanghang1989/detectron2-ResNeSt.git\u001b[39;49;00m\n",
      "\u001b[37m# https://github.com/facebookresearch/detectron2.git \u001b[39;49;00m\n",
      "\u001b[37m# latest version requires torch.jit.is_tracing which is only in torch 1.6.0\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Set a fixed model cache directory. Detectron2 requirement\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mFVCORE_CACHE\u001b[39;49;00m=\u001b[33m\"/tmp\"\u001b[39;49;00m\n",
      "\u001b[37m# set location of training datasetm, Sagemaker containers copy all data from S3 to /opt/ml/input/data/{channels}\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mDETECTRON2_DATASETS\u001b[39;49;00m=\u001b[33m\"/opt/ml/input/data/training\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############# SageMaker section ##############\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mCOPY\u001b[39;49;00m container_training /opt/ml/code\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /opt/ml/code\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# cloning D2 to code dir as we need access to default congigs\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m git clone \u001b[33m'https://github.com/facebookresearch/detectron2.git'\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m \u001b[36mcd\u001b[39;49;00m detectron2 && git checkout be792b9 \n",
      "\u001b[34mRUN\u001b[39;49;00m python -m pip install -e detectron2\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_PROGRAM train_coco.py\n",
      "\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Starts PyTorch distributed framework\u001b[39;49;00m\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m [\u001b[33m\"bash\"\u001b[39;49;00m, \u001b[33m\"-m\"\u001b[39;49;00m, \u001b[33m\"start_with_right_hostname.sh\"\u001b[39;49;00m]\n"
     ]
    }
   ],
   "source": [
    "!pygmentize Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to build your container from this Dockerfile and push it to Amazon Elastic Container Registry using the `build_and_push.sh` script. But you'll need to loging to Sagemaker ECR and your private ECR first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loging to Sagemaker ECR with Deep Learning Containers\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "# loging to your private ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 553020858742.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can push your D2 container to Amazon Elastic Container Registry (ECR) so that we can later pull this container in to our training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ./build_and_push.sh d2-sm-coco distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some algorithm metrics. SageMaker will scrape the logs from our training job and render them in the training job console. The metrics we are defining are pretty standard, you just need to define the regex to find them, feel free to define your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"d2-sm-coco\" # your container name\n",
    "tag = \"distributed\"\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{container}:{tag}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'total_loss', 'Regex': '.*total_loss:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_cls', 'Regex': '.*loss_cls:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_box_reg', 'Regex': '.*loss_box_reg:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_mask', 'Regex': '.*loss_mask:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_rpn_cls', 'Regex': '.*loss_rpn_cls:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_rpn_loc', 'Regex': '.*loss_rpn_loc:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'overall_training_speed',\n",
       "  'Regex': '.*Overall training speed:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'lr', 'Regex': '.*lr:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'iter', 'Regex': '.*iter:\\\\s([0-9\\\\.]+)\\\\s*'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_definitions=[{\n",
    "        \"Name\": \"total_loss\",\n",
    "        \"Regex\": \".*total_loss:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_cls\",\n",
    "        \"Regex\": \".*loss_cls:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_box_reg\",\n",
    "        \"Regex\": \".*loss_box_reg:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_mask\",\n",
    "        \"Regex\": \".*loss_mask:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_rpn_cls\",\n",
    "        \"Regex\": \".*loss_rpn_cls:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_rpn_loc\",\n",
    "        \"Regex\": \".*loss_rpn_loc:\\s([0-9\\\\.]+)\\s*\"\n",
    "    }, \n",
    "    {\n",
    "        \"Name\": \"overall_training_speed\",\n",
    "        \"Regex\": \".*Overall training speed:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"lr\",  \n",
    "        \"Regex\": \".*lr:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"iter\",  \n",
    "        \"Regex\": \".*iter:\\s([0-9\\\\.]+)\\s*\"\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_path = 'metric_defs'\n",
    "\n",
    "with open(metric_path, 'w') as f:\n",
    "    for met in metric_definitions:\n",
    "        f.write(json.dumps(met))\n",
    "        f.write('\\n')\n",
    "        \n",
    "metric_definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Experiments\n",
    "\n",
    "SageMaker experiments lets you organize, track, compare and evaluate machine learning experiments and model versions. We can add experiments tracking to our training jobs using a couple simple hooks. There is a small amount of setup required before we can hook it into our estimators. We first are going to create our tracker and within our tracker, create an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.utils:IMDS ENDPOINT: http://169.254.169.254/\n",
      "INFO:botocore.utils:IMDS ENDPOINT: http://169.254.169.254/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7f826c8735c0>,experiment_name='d2-coco-demo-1603413360',description='Detectron2 training on COCO2017',tags=None,experiment_arn='arn:aws:sagemaker:us-east-1:209419068016:experiment/d2-coco-demo-1603413360',response_metadata={'RequestId': '1ad1074b-55eb-44f2-8340-bf560e11991c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1ad1074b-55eb-44f2-8340-bf560e11991c', 'content-type': 'application/x-amz-json-1.1', 'content-length': '95', 'date': 'Fri, 23 Oct 2020 00:35:59 GMT'}, 'RetryAttempts': 0})\n"
     ]
    }
   ],
   "source": [
    "# create d2 experiment\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "b3sess = boto3.Session()\n",
    "sm = b3sess.client('sagemaker')\n",
    "\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        \"normalization_mean\": 0.1307,\n",
    "        \"normalization_std\": 0.3081,\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "#     tracker.log_input(name=\"d2-dataset\", media_type=\"s3/uri\", value=inputs)\n",
    "\n",
    "d2_experiment = Experiment.create(\n",
    "    experiment_name=f\"d2-coco-demo-{int(time.time())}\", \n",
    "    description=\"Detectron2 training on COCO2017\", \n",
    "    sagemaker_boto_client=sm)\n",
    "print(d2_experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a trial within our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channel_trial_name_map = {}\n",
    "preprocessing_trial_component = tracker.trial_component\n",
    "\n",
    "trial_name = f\"d2-demo-training-job-{int(time.time())}\"\n",
    "d2_trial = Trial.create(\n",
    "    trial_name=trial_name, \n",
    "    experiment_name=d2_experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "hidden_channel_trial_name_map[0] = trial_name\n",
    "\n",
    "# associate the proprocessing trial component with the current trial\n",
    "d2_trial.add_trial_component(preprocessing_trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Hyperparameters\n",
    "\n",
    "Let's set our hyperparameters. For Detectron2, model hyperparameters like learning rate, weight decay, etc. are specified in the configuration file. We however can override those parameters by using \"opts\". Here we are overriding the number of iterations so that our training job completes within roughly an hour. \n",
    "\n",
    "You can also pass in a s3 path to a model checkpoint to start your training from. We can use this to resume training from a previous job or restart spot jobs that timed out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The configuration below will automatically download the below model, to pull in the model locally run this command\n",
    "# !wget https://dl.fbaipublicfiles.com/detectron2/ImageNetPretrained/MSRA/R-101.pkl .\n",
    "\n",
    "iters = 2000\n",
    "hyperparameters = {\"config-file\":\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\", \n",
    "                   #\"local-config-file\" : \"config.yaml\", # if you'd like to supply custom config file, please add it in container_training folder, and provide file name here\n",
    "                   \"resume\":\"True\", # whether to re-use weights from pre-trained model\n",
    "                   \"eval-only\":\"False\", # whether to perform only D2 model evaluation\n",
    "                  # opts are D2 model configuration as defined here: https://detectron2.readthedocs.io/modules/config.html#config-references\n",
    "                  # this is a way to override individual parameters in D2 configuration from Sagemaker API\n",
    "                   \"opts\": f\"SOLVER.MAX_ITER {iters}\",\n",
    "                   \"spot_ckpt\":''\n",
    "                   }\n",
    "\n",
    "with open('hyperparams.json', 'w') as f:\n",
    "    json.dump(hyperparameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anaconda3  Nvidia_Cloud_EULA.pdf  sample-notebooks\t       tools\n",
      "examples   README\t\t  sample-notebooks-1603412981  tutorials\n",
      "LICENSE    SageMaker\t\t  src\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch a Job via Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating training-job with name: 2-nodes-max-iter-2000\n"
     ]
    }
   ],
   "source": [
    "# sessLocal = sagemaker.LocalSession() # can use LocalSession()\n",
    "    \n",
    "# launch using PyTorch estimator\n",
    "d2 = PyTorch(      image_name = image,\n",
    "                   role=role,\n",
    "                   entry_point='/home/ec2-user/SageMaker/detectron2-sagemaker/container_training/train_coco.py',\n",
    "                   train_instance_count=2, \n",
    "                   train_instance_type= 'ml.p3dn.24xlarge',\n",
    "#                     train_instance_type=\"local_gpu\", # use local_gpu for quick troubleshooting\n",
    "                   train_volume_size=100,\n",
    "                   framework_version='1.5.1',\n",
    "                   source_dir='/home/ec2-user/SageMaker/detectron2-sagemaker/',\n",
    "                   output_path=f\"s3://{bucket}/{prefix_output}\",\n",
    "                   metric_definitions = metric_definitions,\n",
    "                   hyperparameters = hyperparameters, \n",
    "                   sagemaker_session=sess,\n",
    "                   )\n",
    "\n",
    "# can launch using generic estimator as well\n",
    "# d2 = sagemaker.estimator.Estimator(image_name=image,\n",
    "#                                    role=role,\n",
    "#                                    train_instance_count=2, \n",
    "#                                    train_instance_type= 'ml.p3.16xlarge',\n",
    "# #                                   train_instance_type=\"local_gpu\", # use local_gpu for quick troubleshooting\n",
    "#                                    train_volume_size=100,\n",
    "#                                    output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "#                                    metric_definitions = metric_definitions,\n",
    "#                                    hyperparameters = hyperparameters, \n",
    "#                                    sagemaker_session=sess,\n",
    "#                                   )\n",
    "\n",
    "d2.fit({'training':f\"s3://{bucket}/train-coco\"},\n",
    "       job_name = f\"2-nodes-max-iter-{iters}\",\n",
    "       wait=False,\n",
    "              experiment_config={\n",
    "            \"TrialName\": d2_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch via CLI\n",
    "\n",
    "We previously launched a training job from our notebook, we can also launch jobs from the command line using the AWS CLI, boto3 (AWS Python SDK), or the SageMaker Python SDK. The below CLI utilizes boto3 to launch jobs. Take a look at the help text below. Note that this CLI expects the Docker image is already built and pushed to ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: launch_coco_train_boto3.py run-d2-sm [-h] [--bucket BUCKET]\n",
      "                                            [--image_name IMAGE_NAME]\n",
      "                                            [--metric_path METRIC_PATH]\n",
      "                                            [--job_name JOB_NAME]\n",
      "                                            [--region REGION]\n",
      "                                            [--prefix_input PREFIX_INPUT]\n",
      "                                            [--prefix_output PREFIX_OUTPUT]\n",
      "                                            [--instance_count INSTANCE_COUNT]\n",
      "                                            [--data_prefix DATA_PREFIX]\n",
      "                                            [--instance_type INSTANCE_TYPE]\n",
      "                                            [--volume_size VOLUME_SIZE]\n",
      "                                            [--use_spot] [--role ROLE]\n",
      "                                            [--max_run_time MAX_RUN_TIME]\n",
      "                                            [--max_wait_time MAX_WAIT_TIME]\n",
      "                                            [--hyperparam_path HYPERPARAM_PATH]\n",
      "\n",
      "    Utility for launching detectron2 training jobs using boto3 create_training_job API.\n",
      "    Has options for launching jobs using spot instances, if launching spot,\n",
      "    please set max_wait_time variable. Check different methods for additional documentation\n",
      "    \n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --bucket BUCKET       s3 bucket for data retrieval and storage of results\n",
      "                        (default: -)\n",
      "  --image_name IMAGE_NAME\n",
      "                        Name of the Docker image to be used for training\n",
      "                        (default: -)\n",
      "  --metric_path METRIC_PATH\n",
      "                        Location for metric definition file (default: -)\n",
      "  --job_name JOB_NAME   Name of the SageMaker training job to launch (default:\n",
      "                        'd2-coco-train')\n",
      "  --region REGION\n",
      "  --prefix_input PREFIX_INPUT\n",
      "  --prefix_output PREFIX_OUTPUT\n",
      "  --instance_count INSTANCE_COUNT\n",
      "                        Number of instances to train on (default: 2)\n",
      "  --data_prefix DATA_PREFIX\n",
      "                        location in s3 for your data (default: 'train-coco')\n",
      "  --instance_type INSTANCE_TYPE\n",
      "                        Type of EC2 instances to train on, for (default:\n",
      "                        'ml.p3.16xlarge')\n",
      "  --volume_size VOLUME_SIZE\n",
      "                        Size of EBS volume attached to instance (default: 100)\n",
      "  --use_spot            Whether to use spot instances for training (default:\n",
      "                        False)\n",
      "  --role ROLE           SageMaker execution role (default: -)\n",
      "  --max_run_time MAX_RUN_TIME\n",
      "  --max_wait_time MAX_WAIT_TIME\n",
      "  --hyperparam_path HYPERPARAM_PATH\n",
      "                        Location for hyperparameters file (default: -)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} launch_coco_train_boto3.py run-d2-sm --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the same hyperparameters and metric definitions defined previously, but feel free to adjust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing data from s3://sagemaker-us-east-1-209419068016/train-coco\n",
      "Job launched!\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} launch_coco_train_boto3.py run-d2-sm --bucket {bucket} --job_name d2-cli-job-boto3 --region {region} \\\n",
    "--metric_path metric_defs --hyperparam_path hyperparams.json --role {role} --image_name {image}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check-d2-sm will check on the status of a given job, and return the job ARN, job status, and hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ARN:  arn:aws:sagemaker:us-east-1:209419068016:training-job/d2-cli-job3\n",
      "Job Status:  Failed\n",
      "Hyperparameters:  {'config-file': 'COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml', 'eval-only': 'False', 'opts': 'SOLVER.MAX_ITER 2000', 'resume': 'True'}\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} launch_coco_train_boto3.py check-d2-sm --job_name d2-cli-job3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Spot Instance with s3\n",
    "\n",
    "To launch a training job using spot instances we just need to add a few additional arguments to our estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use_spot_instances = True\n",
    "train_max_run=21600\n",
    "train_max_wait = 10000 if train_use_spot_instances else None\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_uri = f's3://{bucket}/artifacts/d2-checkpoint-{checkpoint_suffix}/' if train_use_spot_instances else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating training-job with name: 2-nodes-max-iter-2000-spot\n"
     ]
    }
   ],
   "source": [
    "iters = 2000\n",
    "hyperparameters = {\"config-file\":\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\", \n",
    "                   #\"local-config-file\" : \"config.yaml\", # if you'd like to supply custom config file, please add it in container_training folder, and provide file name here\n",
    "                   \"resume\":\"True\", # whether to re-use weights from pre-trained model\n",
    "                   \"eval-only\":\"False\", # whether to perform only D2 model evaluation\n",
    "                  # opts are D2 model configuration as defined here: https://detectron2.readthedocs.io/modules/config.html#config-references\n",
    "                  # this is a way to override individual parameters in D2 configuration from Sagemaker API\n",
    "                   \"opts\": f\"SOLVER.MAX_ITER {iters} SOLVER.BASE_LR 0.002 SOLVER.CHECKPOINT_PERIOD 1000\",\n",
    "                   # try starting a new job from a previous checkpoint by supplying the s3 path in spot_ckpt\n",
    "                   \"spot_ckpt\":''\n",
    "                   }\n",
    "\n",
    "# this time we are going to use the generic estimator. \n",
    "d2 = sagemaker.estimator.Estimator(image,\n",
    "                                   role=role,\n",
    "                                   train_instance_count=2, \n",
    "                                   train_instance_type='ml.p3.2xlarge',\n",
    "                                   train_volume_size=100,\n",
    "                                   output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "                                   metric_definitions = metric_definitions,\n",
    "                                   hyperparameters = hyperparameters, \n",
    "                                   sagemaker_session=sess,\n",
    "                                   train_use_spot_instances=train_use_spot_instances,\n",
    "                                   train_max_run=train_max_run,\n",
    "                                   train_max_wait=train_max_wait,\n",
    "                                   checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "                                  )\n",
    "\n",
    "d2.fit({'training':f\"s3://{bucket}/train-coco\"},\n",
    "       job_name = f\"2-nodes-max-iter-{iters}-spot\",\n",
    "       wait=False,\n",
    "              experiment_config={\n",
    "            \"TrialName\": d2_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>SageMaker.ImageUri</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>config-file</th>\n",
       "      <th>eval-only</th>\n",
       "      <th>opts</th>\n",
       "      <th>resume</th>\n",
       "      <th>sagemaker_container_log_level</th>\n",
       "      <th>sagemaker_enable_cloudwatch_metrics</th>\n",
       "      <th>sagemaker_job_name</th>\n",
       "      <th>sagemaker_program</th>\n",
       "      <th>sagemaker_region</th>\n",
       "      <th>sagemaker_submit_directory</th>\n",
       "      <th>spot_ckpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-nodes-max-iter-2000-demo-p3dn-aws-training-job</td>\n",
       "      <td>Training</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:209419068016:train...</td>\n",
       "      <td>209419068016.dkr.ecr.us-east-1.amazonaws.com/d...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ml.p3dn.24xlarge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_...</td>\n",
       "      <td>\"False\"</td>\n",
       "      <td>\"SOLVER.MAX_ITER 2000\"</td>\n",
       "      <td>\"True\"</td>\n",
       "      <td>20.0</td>\n",
       "      <td>false</td>\n",
       "      <td>\"2-nodes-max-iter-2000-demo-p3dn\"</td>\n",
       "      <td>\"/home/ec2-user/SageMaker/detectron2-sagemaker...</td>\n",
       "      <td>\"us-east-1\"</td>\n",
       "      <td>\"s3://sagemaker-us-east-1-209419068016/2-nodes...</td>\n",
       "      <td>\"\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 TrialComponentName DisplayName  \\\n",
       "0  2-nodes-max-iter-2000-demo-p3dn-aws-training-job    Training   \n",
       "\n",
       "                                           SourceArn  \\\n",
       "0  arn:aws:sagemaker:us-east-1:209419068016:train...   \n",
       "\n",
       "                                  SageMaker.ImageUri  SageMaker.InstanceCount  \\\n",
       "0  209419068016.dkr.ecr.us-east-1.amazonaws.com/d...                      2.0   \n",
       "\n",
       "  SageMaker.InstanceType  SageMaker.VolumeSizeInGB  \\\n",
       "0       ml.p3dn.24xlarge                     100.0   \n",
       "\n",
       "                                         config-file eval-only  \\\n",
       "0  \"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_...   \"False\"   \n",
       "\n",
       "                     opts  resume  sagemaker_container_log_level  \\\n",
       "0  \"SOLVER.MAX_ITER 2000\"  \"True\"                           20.0   \n",
       "\n",
       "  sagemaker_enable_cloudwatch_metrics                 sagemaker_job_name  \\\n",
       "0                               false  \"2-nodes-max-iter-2000-demo-p3dn\"   \n",
       "\n",
       "                                   sagemaker_program sagemaker_region  \\\n",
       "0  \"/home/ec2-user/SageMaker/detectron2-sagemaker...      \"us-east-1\"   \n",
       "\n",
       "                          sagemaker_submit_directory spot_ckpt  \n",
       "0  \"s3://sagemaker-us-east-1-209419068016/2-nodes...        \"\"  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_expression = {\n",
    "    \"Filters\":[\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=Session(b3sess, sm), \n",
    "    experiment_name=d2_experiment.experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.test:accuracy.max\",\n",
    "    sort_order=\"Descending\",\n",
    "    metric_names=['test:accuracy'],\n",
    ")\n",
    "\n",
    "trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also check the status of your jobs using boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "sm_client.describe_training_job(TrainingJobName='d2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
