{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Detectron2 SageMaker Demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.pytorch import estimator, PyTorchModel, PyTorchPredictor, PyTorch\n",
    "\n",
    "# get our execution role giving us permissions to do things like launch training jobs\n",
    "role = get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "sess = sagemaker.Session() # can use LocalSession() to run container locally\n",
    "\n",
    "bucket = 'privisaa-bucket-2' # sess.default_bucket()\n",
    "region = \"us-east-1\"\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "prefix_input = 'detectron2-input'\n",
    "prefix_output = 'detectron2-output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install SageMaker Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-experiments==0.1.24 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (0.1.24)\n",
      "Requirement already satisfied: boto3>=1.12.8 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sagemaker-experiments==0.1.24) (1.15.8)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments==0.1.24) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments==0.1.24) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.8 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments==0.1.24) (1.18.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.19.0,>=1.18.8->boto3>=1.12.8->sagemaker-experiments==0.1.24) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.19.0,>=1.18.8->boto3>=1.12.8->sagemaker-experiments==0.1.24) (1.25.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.8->boto3>=1.12.8->sagemaker-experiments==0.1.24) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install sagemaker-experiments==0.1.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data for training\n",
    "\n",
    "We are grabbing data from COCO, decompressing the data, and then sending it to s3. In this notebook we have two examples, one using s3 and one using EFS, if you want to utilize the EFS example, you'll need to mount your EFS volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create stage directory: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32\n",
      "--2020-10-15 00:24:32--  http://images.cocodataset.org/zips/train2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.44.140\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.44.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19336861798 (18G) [application/zip]\n",
      "Saving to: ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/train2017.zip’\n",
      "\n",
      "/home/ec2-user/Sage 100%[===================>]  18.01G  74.1MB/s    in 3m 59s  \n",
      "\n",
      "2020-10-15 00:28:32 (77.0 MB/s) - ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/train2017.zip’ saved [19336861798/19336861798]\n",
      "\n",
      "Extracting /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/train2017.zip\n",
      "============================================================================================================================================================================================================================================Done.\n",
      "--2020-10-15 00:30:14--  http://images.cocodataset.org/zips/val2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.33.163\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.33.163|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 815585330 (778M) [application/zip]\n",
      "Saving to: ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/val2017.zip’\n",
      "\n",
      "/home/ec2-user/Sage 100%[===================>] 777.80M  95.0MB/s    in 8.1s    \n",
      "\n",
      "2020-10-15 00:30:22 (95.7 MB/s) - ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/val2017.zip’ saved [815585330/815585330]\n",
      "\n",
      "Extracting /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/val2017.zip\n",
      "==========Done.\n",
      "--2020-10-15 00:30:27--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.249.12\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.249.12|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252907541 (241M) [application/zip]\n",
      "Saving to: ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations_trainval2017.zip’\n",
      "\n",
      "/home/ec2-user/Sage 100%[===================>] 241.19M  96.0MB/s    in 2.5s    \n",
      "\n",
      "2020-10-15 00:30:30 (96.0 MB/s) - ‘/home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations_trainval2017.zip’ saved [252907541/252907541]\n",
      "\n",
      "Archive:  /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations_trainval2017.zip\n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/instances_train2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/instances_val2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/captions_train2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/captions_val2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/person_keypoints_train2017.json  \n",
      "  inflating: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32/annotations/person_keypoints_val2017.json  \n",
      "Thu Oct 15 00:30:36 UTC 2020: Uploading extracted files to s3://privisaa-bucket-2/coco2017 [ eta 12 minutes ]\n",
      "================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================Done.\n",
      "Delete stage directory: /home/ec2-user/SageMaker/coco-2017-2020-10-15-00-24-32\n",
      "Success.\n"
     ]
    }
   ],
   "source": [
    "! ./upload_coco2017_to_s3.sh {bucket} coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using EFS, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash mount_efs.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash send_data_to_efs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Docker image to registry\n",
    "\n",
    "For this training, we'll extend the [Sagemaker PyTorch Container](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html) with Detectron2 dependencies (using official [D2 Dockerfile](https://github.com/facebookresearch/detectron2/blob/master/docker/Dockerfile)) as baseline. See Dockerfile below.\n",
    "\n",
    "You are in no means limited to using our containers, for examples of jobs using outside containers see:\n",
    "\n",
    "[SageMaker Nvidia NGC Examples ](https://github.com/aws-samples/amazon-sagemaker-nvidia-ngc-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Build an image of Detectron2 that can do distributing training on Amazon Sagemaker \u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# using Sagemaker PyTorch container as base image\u001b[39;49;00m\n",
      "\u001b[37m# https://github.com/aws/sagemaker-pytorch-container/blob/master/docker/1.4.0/py3/Dockerfile.gpu\u001b[39;49;00m\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.1-gpu-py36-cu101-ubuntu16.04\u001b[39;49;00m\n",
      "\u001b[34mLABEL\u001b[39;49;00m \u001b[31mauthor\u001b[39;49;00m=\u001b[33m\"vadimd@amazon.com\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############# Installing latest builds ############\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# This is to fix issue: https://github.com/pytorch/vision/issues/1489\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install --upgrade --force-reinstall \u001b[31mtorch\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.5.1 \u001b[31mtorchvision\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.6.1 cython\n",
      "\u001b[37m# RUN pip install torchvision==0.7.0\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############# D2 section ##############\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# installing dependecies for D2 https://github.com/facebookresearch/detectron2/blob/master/docker/Dockerfile\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install \u001b[33m'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install \u001b[33m'git+https://github.com/facebookresearch/fvcore'\u001b[39;49;00m \n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mFORCE_CUDA\u001b[39;49;00m=\u001b[33m\"1\"\u001b[39;49;00m\n",
      "\u001b[37m# Build D2 only for Volta architecture - V100 chips (ml.p3 AWS instances)\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mTORCH_CUDA_ARCH_LIST\u001b[39;49;00m=\u001b[33m\"Volta\"\u001b[39;49;00m \n",
      "\n",
      "\u001b[37m# Build D2 from latest sources\u001b[39;49;00m\n",
      "\u001b[37m# RUN pip install 'git+https://github.com/zhanghang1989/detectron2-ResNeSt.git'\u001b[39;49;00m\n",
      "\u001b[37m# https://github.com/zhanghang1989/detectron2-ResNeSt.git\u001b[39;49;00m\n",
      "\u001b[37m# https://github.com/facebookresearch/detectron2.git \u001b[39;49;00m\n",
      "\u001b[37m# latest version requires torch.jit.is_tracing which is only in torch 1.6.0\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Set a fixed model cache directory. Detectron2 requirement\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mFVCORE_CACHE\u001b[39;49;00m=\u001b[33m\"/tmp\"\u001b[39;49;00m\n",
      "\u001b[37m# set location of training datasetm, Sagemaker containers copy all data from S3 to /opt/ml/input/data/{channels}\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m \u001b[31mDETECTRON2_DATASETS\u001b[39;49;00m=\u001b[33m\"/opt/ml/input/data/training\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############# SageMaker section ##############\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mCOPY\u001b[39;49;00m container_training /opt/ml/code\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /opt/ml/code\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# cloning D2 to code dir as we need access to default congigs\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m git clone \u001b[33m'https://github.com/facebookresearch/detectron2.git'\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m \u001b[36mcd\u001b[39;49;00m detectron2 && git checkout be792b9 \n",
      "\u001b[34mRUN\u001b[39;49;00m python -m pip install -e detectron2\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_PROGRAM train_coco.py\n",
      "\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Starts PyTorch distributed framework\u001b[39;49;00m\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m [\u001b[33m\"bash\"\u001b[39;49;00m, \u001b[33m\"-m\"\u001b[39;49;00m, \u001b[33m\"start_with_right_hostname.sh\"\u001b[39;49;00m]\n"
     ]
    }
   ],
   "source": [
    "!pygmentize Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to build your container from this Dockerfile and push it to Amazon Elastic Container Registry using the `build_and_push.sh` script. But you'll need to loging to Sagemaker ECR and your private ECR first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "# loging to Sagemaker ECR with Deep Learning Containers\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "# loging to your private ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 553020858742.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can push your D2 container to Amazon Elastic Container Registry (ECR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  511.6MB\n",
      "Step 1/18 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.1-gpu-py36-cu101-ubuntu16.04\n",
      " ---> 2aa16a1b866d\n",
      "Step 2/18 : LABEL author=\"vadimd@amazon.com\"\n",
      " ---> Using cache\n",
      " ---> 5e2766146cbe\n",
      "Step 3/18 : RUN pip install --upgrade --force-reinstall torch==1.5.1 torchvision==0.6.1 cython\n",
      " ---> Using cache\n",
      " ---> 24840acf45f9\n",
      "Step 4/18 : RUN pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
      " ---> Using cache\n",
      " ---> 07b33cfeee9a\n",
      "Step 5/18 : RUN pip install 'git+https://github.com/facebookresearch/fvcore'\n",
      " ---> Using cache\n",
      " ---> 17df2af93165\n",
      "Step 6/18 : ENV FORCE_CUDA=\"1\"\n",
      " ---> Using cache\n",
      " ---> 3d6698291fd7\n",
      "Step 7/18 : ENV TORCH_CUDA_ARCH_LIST=\"Volta\"\n",
      " ---> Using cache\n",
      " ---> d78460d1df58\n",
      "Step 8/18 : ENV FVCORE_CACHE=\"/tmp\"\n",
      " ---> Using cache\n",
      " ---> 646f1ef9d106\n",
      "Step 9/18 : ENV DETECTRON2_DATASETS=\"/opt/ml/input/data/training\"\n",
      " ---> Using cache\n",
      " ---> 3e3f0346658b\n",
      "Step 10/18 : COPY container_training /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 10ca38c68944\n",
      "Step 11/18 : WORKDIR /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 9ec96ce62166\n",
      "Step 12/18 : RUN git clone 'https://github.com/facebookresearch/detectron2.git'\n",
      " ---> Using cache\n",
      " ---> bc7e5827f31b\n",
      "Step 13/18 : RUN cd detectron2 && git checkout be792b9\n",
      " ---> Using cache\n",
      " ---> 2cd2ccd88e25\n",
      "Step 14/18 : RUN python -m pip install -e detectron2\n",
      " ---> Using cache\n",
      " ---> 3355742b6348\n",
      "Step 15/18 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 4fc2784273aa\n",
      "Step 16/18 : ENV SAGEMAKER_PROGRAM train_coco.py\n",
      " ---> Using cache\n",
      " ---> 1efea05db2b6\n",
      "Step 17/18 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 634c877f9121\n",
      "Step 18/18 : ENTRYPOINT [\"bash\", \"-m\", \"start_with_right_hostname.sh\"]\n",
      " ---> Using cache\n",
      " ---> 7b52866133e4\n",
      "Successfully built 7b52866133e4\n",
      "Successfully tagged d2-sm-coco:latest\n",
      "The push refers to repository [209419068016.dkr.ecr.us-east-1.amazonaws.com/d2-sm-coco]\n",
      "\n",
      "\u001b[1B7e6e230c: Preparing \n",
      "\u001b[1Bd3858370: Preparing \n",
      "\u001b[1B5a3b652d: Preparing \n",
      "\u001b[1Bd774e78d: Preparing \n",
      "\u001b[1B4e3e6893: Preparing \n",
      "\u001b[1B31a3265c: Preparing \n",
      "\u001b[1B64232b07: Preparing \n",
      "\u001b[1Bd1449436: Preparing \n",
      "\u001b[1B489ec215: Preparing \n",
      "\u001b[1B211bfa77: Preparing \n",
      "\u001b[1Bb2eeb3a9: Preparing \n",
      "\u001b[1Ba0755a7c: Preparing \n",
      "\u001b[1B20d289c4: Preparing \n",
      "\u001b[1Beb6d9aa4: Preparing \n",
      "\u001b[1B826f51f5: Preparing \n",
      "\u001b[1Bf0501057: Preparing \n",
      "\u001b[1B1bafa536: Preparing \n",
      "\u001b[1Be87dd341: Preparing \n",
      "\u001b[1B80613f50: Preparing \n",
      "\u001b[14B4232b07: Waiting g \n",
      "\u001b[14B1449436: Waiting g \n",
      "\u001b[1B27d48048: Preparing \n",
      "\u001b[1B0c7dc417: Preparing \n",
      "\u001b[14B2eeb3a9: Waiting g \n",
      "\u001b[16B11bfa77: Waiting g \n",
      "\u001b[1B48c6969a: Preparing \n",
      "\u001b[1B7f090df2: Preparing \n",
      "\u001b[14B26f51f5: Waiting g \n",
      "\u001b[1Bdadd4466: Preparing \n",
      "\u001b[9B27d48048: Waiting g \n",
      "\u001b[11Baceafa5: Waiting g \n",
      "\u001b[13B97b3058: Waiting g \n",
      "\u001b[1B9683cb41: Preparing \n",
      "\u001b[17B87dd341: Waiting g \n",
      "\u001b[10B8c6969a: Waiting g \n",
      "\u001b[9B20da503d: Waiting g \n",
      "\u001b[2Be637fbff: Layer already exists \u001b[31A\u001b[2K\u001b[28A\u001b[2K\u001b[23A\u001b[2K\u001b[18A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[3A\u001b[2Kdistributed: digest: sha256:68539261d4c7bf0ea10fa68e84a116d632d874919cc98135eb10c53883580af0 size: 8100\n"
     ]
    }
   ],
   "source": [
    "! ./build_and_push.sh d2-sm-coco distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a variation on the main container designed to work with EFS instead of s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  511.6MB\n",
      "Step 1/17 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.0-gpu-py36-cu101-ubuntu16.04\n",
      " ---> 47cd15520b75\n",
      "Step 2/17 : LABEL author=\"vadimd@amazon.com\"\n",
      " ---> Using cache\n",
      " ---> c7249177e518\n",
      "Step 3/17 : RUN pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
      " ---> Using cache\n",
      " ---> d83fd986a49e\n",
      "Step 4/17 : RUN pip install 'git+https://github.com/facebookresearch/fvcore'\n",
      " ---> Using cache\n",
      " ---> a540b481c57c\n",
      "Step 5/17 : ENV FORCE_CUDA=\"1\"\n",
      " ---> Using cache\n",
      " ---> e6686d2a4ec6\n",
      "Step 6/17 : ENV TORCH_CUDA_ARCH_LIST=\"Volta\"\n",
      " ---> Using cache\n",
      " ---> c3d011fc9e21\n",
      "Step 7/17 : ENV FVCORE_CACHE=\"/tmp\"\n",
      " ---> Using cache\n",
      " ---> 96c48ab9df44\n",
      "Step 8/17 : ENV DETECTRON2_DATASETS=\"/opt/ml/input/data/train\"\n",
      " ---> Using cache\n",
      " ---> cd61b288f019\n",
      "Step 9/17 : COPY container_training /opt/ml/code\n",
      " ---> 9cf5053f94ad\n",
      "Step 10/17 : WORKDIR /opt/ml/code\n",
      " ---> Running in a0bb57caae3a\n",
      "Removing intermediate container a0bb57caae3a\n",
      " ---> 377ad143e4e7\n",
      "Step 11/17 : RUN git clone 'https://github.com/facebookresearch/detectron2.git'\n",
      " ---> Running in 260bf141493e\n",
      "\u001b[91mCloning into 'detectron2'...\n",
      "\u001b[0mRemoving intermediate container 260bf141493e\n",
      " ---> 5f507ac03f63\n",
      "Step 12/17 : RUN cd detectron2 && git checkout be792b9\n",
      " ---> Running in a94e98b88ce8\n",
      "\u001b[91mNote: checking out 'be792b9'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "\u001b[0m\u001b[91mHEAD is now at be792b9... Make 'ROIAlign' & 'ROIAlignV2' version of ROIPooler scriptable.\n",
      "\u001b[0mRemoving intermediate container a94e98b88ce8\n",
      " ---> 1242c6978ee1\n",
      "Step 13/17 : RUN python -m pip install -e detectron2\n",
      " ---> Running in c9b99f9f1ce8\n",
      "Obtaining file:///opt/ml/code/detectron2\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.2) (1.1.0)\n",
      "Requirement already satisfied: Pillow>=7.0 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.2) (7.1.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.2) (0.1.8)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.6/site-packages (from detectron2==0.2) (0.8.7)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.6/site-packages (from detectron2==0.2) (1.4.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from detectron2==0.2) (3.2.1)\n",
      "Collecting mock\n",
      "  Downloading mock-4.0.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.2) (4.42.1)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Requirement already satisfied: fvcore>=0.1.1 in /opt/conda/lib/python3.6/site-packages (from detectron2==0.2) (0.1.2)\n",
      "Collecting pycocotools>=2.0.1\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from detectron2==0.2) (0.17.1)\n",
      "Collecting pydot\n",
      "  Downloading pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from yacs>=0.1.6->detectron2==0.2) (5.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.2) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.2) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.2) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.2) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.6/site-packages (from matplotlib->detectron2==0.2) (1.16.4)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.2) (46.1.3.post20200330)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.2) (1.0.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.22.1-py2.py3-none-any.whl (114 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.2) (3.11.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.2) (2.22.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.2) (0.34.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.2-py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->detectron2==0.2) (1.14.0)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.6/site-packages (from fvcore>=0.1.1->detectron2==0.2) (2.0.0)\n",
      "Requirement already satisfied: cython>=0.27.3 in /opt/conda/lib/python3.6/site-packages (from pycocotools>=2.0.1->detectron2==0.2) (0.29.12)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2==0.2) (3.4.2)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.2) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.2) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.2) (2.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard->detectron2==0.2) (1.6.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard->detectron2==0.2) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2==0.2) (3.1.0)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py): started\n",
      "  Building wheel for pycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp36-cp36m-linux_x86_64.whl size=288532 sha256=4b4552e54239f4945051bb5eff8cc71f83f4acc969d77c6c6a5ae1d1d40d18da\n",
      "  Stored in directory: /root/.cache/pip/wheels/d8/c2/ba/8f5306f921c2e79ad7b09effdfed6bd966cfcf8c6fe55422d6\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: mock, grpcio, cachetools, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, tensorboard-plugin-wit, markdown, tensorboard, pycocotools, pydot, detectron2\n",
      "  Attempting uninstall: pycocotools\n",
      "    Found existing installation: pycocotools 2.0\n",
      "    Uninstalling pycocotools-2.0:\n",
      "      Successfully uninstalled pycocotools-2.0\n",
      "  Running setup.py develop for detectron2\n",
      "Successfully installed absl-py-0.10.0 cachetools-4.1.1 detectron2 google-auth-1.22.1 google-auth-oauthlib-0.4.1 grpcio-1.32.0 markdown-3.3.2 mock-4.0.2 oauthlib-3.1.0 pyasn1-modules-0.2.8 pycocotools-2.0.2 pydot-1.4.1 requests-oauthlib-1.3.0 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0\n",
      "\u001b[91mWARNING: You are using pip version 20.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container c9b99f9f1ce8\n",
      " ---> 2ea0f2fd509c\n",
      "Step 14/17 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Running in 750ff25ca539\n",
      "Removing intermediate container 750ff25ca539\n",
      " ---> a99fb93f3f80\n",
      "Step 15/17 : ENV SAGEMAKER_PROGRAM train_coco_efs.py\n",
      " ---> Running in a079832f516b\n",
      "Removing intermediate container a079832f516b\n",
      " ---> 4caa930d89a9\n",
      "Step 16/17 : WORKDIR /\n",
      " ---> Running in 152b1e067cb9\n",
      "Removing intermediate container 152b1e067cb9\n",
      " ---> ae7a922ed275\n",
      "Step 17/17 : ENTRYPOINT [\"bash\", \"-m\", \"start_with_right_hostname.sh\"]\n",
      " ---> Running in 7a21538c64a3\n",
      "Removing intermediate container 7a21538c64a3\n",
      " ---> b7d1328b85c5\n",
      "Successfully built b7d1328b85c5\n",
      "Successfully tagged d2-sm-coco2:latest\n",
      "The push refers to repository [209419068016.dkr.ecr.us-east-1.amazonaws.com/d2-sm-coco2]\n",
      "\n",
      "\u001b[1Bce41b26b: Preparing \n",
      "\u001b[1B17787b8e: Preparing \n",
      "\u001b[1B24568a35: Preparing \n",
      "\u001b[1B3e3a2a70: Preparing \n",
      "\u001b[1B6e8bcdb2: Preparing \n",
      "\u001b[1Bd6e642bf: Preparing \n",
      "\u001b[1Bdc6eccf1: Preparing \n",
      "\u001b[1Be8cb8ead: Preparing \n",
      "\u001b[1B18b6f784: Preparing \n",
      "\u001b[1Bbe96fc82: Preparing \n",
      "\u001b[1B2b332e53: Preparing \n",
      "\u001b[1Bc6e1c93f: Preparing \n",
      "\u001b[1B8af0cced: Preparing \n",
      "\u001b[1Ba1e058e6: Preparing \n",
      "\u001b[1Ba7e2d141: Preparing \n",
      "\u001b[1B891256c7: Preparing \n",
      "\u001b[1B4275da12: Preparing \n",
      "\u001b[1B8601ef26: Preparing \n",
      "\u001b[14B6e642bf: Waiting g \n",
      "\u001b[1Be3aaa392: Preparing \n",
      "\u001b[11Bb332e53: Waiting g \n",
      "\u001b[16Bc6eccf1: Waiting g \n",
      "\u001b[16B8cb8ead: Waiting g \n",
      "\u001b[12Baf0cced: Waiting g \n",
      "\u001b[8B8601ef26: Waiting g \n",
      "\u001b[11B91256c7: Waiting g \n",
      "\u001b[13B7e2d141: Waiting g \n",
      "\u001b[1B9e8d85dd: Preparing \n",
      "\u001b[6Bab783b62: Waiting g \n",
      "\u001b[1B9683cb41: Preparing \n",
      "\u001b[5B74e50f52: Waiting g \n",
      "\u001b[4B74ebe255: Waiting g \n",
      "\u001b[3B42719515: Waiting g \n",
      "\u001b[31Be3a2a70: Pushed   178.8MB/178.8MB\u001b[31A\u001b[2K\u001b[28A\u001b[2K\u001b[33A\u001b[2K\u001b[26A\u001b[2K\u001b[25A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[22A\u001b[2K\u001b[32A\u001b[2K\u001b[20A\u001b[2K\u001b[34A\u001b[2K\u001b[17A\u001b[2K\u001b[34A\u001b[2K\u001b[13A\u001b[2K\u001b[34A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[34A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2Kdistributed: digest: sha256:c9ae052a1cd7b05baca970336a74cd1970b115133a41c9e144c25f22cf4d6b76 size: 7473\n"
     ]
    }
   ],
   "source": [
    "! ./build_and_push.sh d2-sm-coco2 distributed Dockerfile.efs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some algorithm metrics. SageMaker will scrape the logs from our training job and render them in the training job console. The metrics we are defining are pretty standard, you just need to define the regex to find them, feel free to define your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"d2-sm-coco\" # your container name\n",
    "tag = \"distributed\"\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{container}:{tag}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'total_loss', 'Regex': '.*total_loss:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_cls', 'Regex': '.*loss_cls:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_box_reg', 'Regex': '.*loss_box_reg:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_mask', 'Regex': '.*loss_mask:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_rpn_cls', 'Regex': '.*loss_rpn_cls:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'loss_rpn_loc', 'Regex': '.*loss_rpn_loc:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'overall_training_speed',\n",
       "  'Regex': '.*Overall training speed:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'lr', 'Regex': '.*lr:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       " {'Name': 'iter', 'Regex': '.*iter:\\\\s([0-9\\\\.]+)\\\\s*'}]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_definitions=[{\n",
    "        \"Name\": \"total_loss\",\n",
    "        \"Regex\": \".*total_loss:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_cls\",\n",
    "        \"Regex\": \".*loss_cls:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_box_reg\",\n",
    "        \"Regex\": \".*loss_box_reg:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_mask\",\n",
    "        \"Regex\": \".*loss_mask:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_rpn_cls\",\n",
    "        \"Regex\": \".*loss_rpn_cls:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_rpn_loc\",\n",
    "        \"Regex\": \".*loss_rpn_loc:\\s([0-9\\\\.]+)\\s*\"\n",
    "    }, \n",
    "    {\n",
    "        \"Name\": \"overall_training_speed\",\n",
    "        \"Regex\": \".*Overall training speed:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"lr\",  \n",
    "        \"Regex\": \".*lr:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"iter\",  \n",
    "        \"Regex\": \".*iter:\\s([0-9\\\\.]+)\\s*\"\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_path = 'metric_defs'\n",
    "\n",
    "with open(metric_path, 'w') as f:\n",
    "    for met in metric_definitions:\n",
    "        f.write(json.dumps(met))\n",
    "        f.write('\\n')\n",
    "        \n",
    "metric_definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Experiments\n",
    "\n",
    "SageMaker experiments needs some setup before we can hook it into our estimators. We first are going to create our tracker and within our tracker, create an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7f3f88ed37b8>,experiment_name='d2-coco-demo-1603299483',description='Detectron2 training on COCO2017',tags=None,experiment_arn='arn:aws:sagemaker:us-east-1:209419068016:experiment/d2-coco-demo-1603299483',response_metadata={'RequestId': '9f26cc0d-76ce-4af3-9220-c6b2300c174e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '9f26cc0d-76ce-4af3-9220-c6b2300c174e', 'content-type': 'application/x-amz-json-1.1', 'content-length': '95', 'date': 'Wed, 21 Oct 2020 16:58:03 GMT'}, 'RetryAttempts': 0})\n"
     ]
    }
   ],
   "source": [
    "# create d2 experiment\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "b3sess = boto3.Session()\n",
    "sm = b3sess.client('sagemaker')\n",
    "\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        \"normalization_mean\": 0.1307,\n",
    "        \"normalization_std\": 0.3081,\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "#     tracker.log_input(name=\"d2-dataset\", media_type=\"s3/uri\", value=inputs)\n",
    "\n",
    "d2_experiment = Experiment.create(\n",
    "    experiment_name=f\"d2-coco-demo-{int(time.time())}\", \n",
    "    description=\"Detectron2 training on COCO2017\", \n",
    "    sagemaker_boto_client=sm)\n",
    "print(d2_experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a trial within our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channel_trial_name_map = {}\n",
    "preprocessing_trial_component = tracker.trial_component\n",
    "\n",
    "trial_name = f\"d2-demo-training-job-{int(time.time())}\"\n",
    "d2_trial = Trial.create(\n",
    "    trial_name=trial_name, \n",
    "    experiment_name=d2_experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "hidden_channel_trial_name_map[0] = trial_name\n",
    "\n",
    "# associate the proprocessing trial component with the current trial\n",
    "d2_trial.add_trial_component(preprocessing_trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Hyperparameters\n",
    "\n",
    "Let's set our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/detectron2/ImageNetPretrained/MSRA/R-101.pkl .\n",
    "# !aws s3 cp R-101.pkl s3://privisaa-bucket-2/models/mask_rcnn_R_101_C4_3x/R-101.pkl\n",
    "\n",
    "hyperparameters = {\"config-file\":\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\", \n",
    "                   #\"local-config-file\" : \"config.yaml\", # if you'd like to supply custom config file, please add it in container_training folder, and provide file name here\n",
    "                   \"resume\":\"True\", # whether to re-use weights from pre-trained model\n",
    "                   \"eval-only\":\"False\", # whether to perform only D2 model evaluation\n",
    "                  # opts are D2 model configuration as defined here: https://detectron2.readthedocs.io/modules/config.html#config-references\n",
    "                  # this is a way to override individual parameters in D2 configuration from Sagemaker API\n",
    "                   \"opts\": \"SOLVER.MAX_ITER 1000\",\n",
    "                   \"spot_ckpt\":''\n",
    "                   }\n",
    "\n",
    "with open('hyperparams.json', 'w') as f:\n",
    "    json.dump(hyperparameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch a Job via Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating training-job with name: 2-nodes-max-iter-1000-demo2-p3dn\n"
     ]
    }
   ],
   "source": [
    "# sessLocal = sagemaker.LocalSession() # can use LocalSession()\n",
    "    \n",
    "d2 = PyTorch(      image_name = image,\n",
    "                   role=role,\n",
    "                   entry_point='/home/ec2-user/SageMaker/detectron2-sagemaker/container_training/train_coco.py',\n",
    "                   train_instance_count=2, \n",
    "                   train_instance_type= 'ml.p3dn.24xlarge',\n",
    "#                     train_instance_type=\"local_gpu\", # use local_gpu for quick troubleshooting\n",
    "                   train_volume_size=100,\n",
    "                   framework_version='1.5.1',\n",
    "                   source_dir='/home/ec2-user/SageMaker/detectron2-sagemaker/',\n",
    "                   output_path=f\"s3://{bucket}/{prefix_output}\",\n",
    "                   metric_definitions = metric_definitions,\n",
    "                   hyperparameters = hyperparameters, \n",
    "                   sagemaker_session=sess,\n",
    "                   profiler_config=profiler_config)\n",
    "\n",
    "# d2 = sagemaker.estimator.Estimator(image_name=image,\n",
    "#                                    role=role,\n",
    "#                                    train_instance_count=2, \n",
    "#                                    train_instance_type= 'ml.p3.16xlarge',\n",
    "# #                                   train_instance_type=\"local_gpu\", # use local_gpu for quick troubleshooting\n",
    "#                                    train_volume_size=100,\n",
    "#                                    output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "#                                    metric_definitions = metric_definitions,\n",
    "#                                    hyperparameters = hyperparameters, \n",
    "#                                    sagemaker_session=sess,\n",
    "#                                   profiler_config=profiler_config)\n",
    "\n",
    "d2.fit({'training':f\"s3://{bucket}/train-coco\"},\n",
    "       job_name = \"2-nodes-max-iter-1000-demo2-p3dn\",\n",
    "       wait=False,\n",
    "              experiment_config={\n",
    "            \"TrialName\": d2_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFS Setup\n",
    "\n",
    "In order to use EFS with SageMaker training we need to setup a pointer to our file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFS file-system-id: fs-d120c724\n",
      "EFS file-system data input path: /training\n",
      "Creating log directory on EFS: /training\n"
     ]
    }
   ],
   "source": [
    "# for EFS\n",
    "\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "# Specify EFS ile system id.\n",
    "file_system_id = 'fs----------'\n",
    "print(f\"EFS file-system-id: {file_system_id}\")\n",
    "\n",
    "# Specify directory path for input data on the file system. \n",
    "# You need to provide normalized and absolute path below.\n",
    "file_system_directory_path = '/training' # sagemaker/input/train\n",
    "print(f'EFS file-system data input path: {file_system_directory_path}')\n",
    "\n",
    "home_dir='/home/ec2-user/SageMaker/' #os.environ['HOME']\n",
    "local_efs_path = os.path.join(home_dir, 'efs', file_system_directory_path) # 'efs',\n",
    "print(f\"Creating log directory on EFS: {local_efs_path}\")\n",
    "\n",
    "# Specify the access mode of the mount of the directory associated with the file system. \n",
    "# Directory must be mounted  'ro'(read-only).\n",
    "file_system_access_mode = 'ro'\n",
    "\n",
    "# Specify your file system type\n",
    "file_system_type = 'EFS'\n",
    "\n",
    "train = FileSystemInput(file_system_id=file_system_id,\n",
    "                                    file_system_type=file_system_type,\n",
    "                                    directory_path=file_system_directory_path,\n",
    "                                    file_system_access_mode=file_system_access_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch a job using Spot and EFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use_spot_instances = True\n",
    "train_max_run=10000\n",
    "train_max_wait = 10000 if train_use_spot_instances else None\n",
    "\n",
    "import uuid\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_uri = 's3://{}/artifacts/d2-efs-checkpoint-{}/'.format(bucket, checkpoint_suffix) if train_use_spot_instances else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup separate trial for EFS training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channel_trial_name_map = {}\n",
    "preprocessing_trial_component = tracker.trial_component\n",
    "\n",
    "trial_name = f\"d2-demo-efs-training-job-{int(time.time())}\"\n",
    "d2_trial = Trial.create(\n",
    "    trial_name=trial_name, \n",
    "    experiment_name=d2_experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "hidden_channel_trial_name_map[0] = trial_name\n",
    "\n",
    "# associate the proprocessing trial component with the current trial\n",
    "d2_trial.add_trial_component(preprocessing_trial_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"config-file\":\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\", \n",
    "                   #\"local-config-file\" : \"config.yaml\", # if you'd like to supply custom config file, please add it in container_training folder, and provide file name here\n",
    "                   \"resume\":\"True\", # whether to re-use weights from pre-trained model\n",
    "                   \"eval-only\":\"False\", # whether to perform only D2 model evaluation\n",
    "                  # opts are D2 model configuration as defined here: https://detectron2.readthedocs.io/modules/config.html#config-references\n",
    "                  # this is a way to override individual parameters in D2 configuration from Sagemaker API\n",
    "                   \"opts\": \"SOLVER.MAX_ITER 5000 SOLVER.BASE_LR 0.002 SOLVER.CHECKPOINT_PERIOD 1000\",\n",
    "                   \"spot_ckpt\":'s3://privisaa-bucket-2/artifacts/d2-efs-checkpoint-57a35863/model_final.pth'\n",
    "                   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Training Job: d2-efstraining-spotckpt-recovery2-1603306370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: d2-efstraining-spotckpt-recovery2-1603306370\n"
     ]
    }
   ],
   "source": [
    "# Give Amazon SageMaker Training Jobs Access to FileSystem Resources in Your Amazon VPC.\n",
    "\n",
    "security_group_ids = ['sg-317ad11a'] # ['sg-xxxxxxxx']\n",
    "subnets =  ['subnet-9e445ef9'] # ['subnet-xxxxxxx', 'subnet-xxxxxxx', 'subnet-xxxxxxx']\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "d2_efs = PyTorch(image_name = \"209419068016.dkr.ecr.us-east-1.amazonaws.com/d2-sm-coco2:distributed\",\n",
    "                                   role=role,\n",
    "                                   entry_point='/home/ec2-user/SageMaker/detectron2-sagemaker/container_training/train_coco_efs.py',\n",
    "                                   train_instance_count=2, \n",
    "                                   train_instance_type= 'ml.p3.8xlarge',\n",
    "#                                   train_instance_type=\"local_gpu\", # use local_gpu for quick troubleshooting\n",
    "                                   train_volume_size=100,\n",
    "                                   framework_version='1.5.0',\n",
    "                                   source_dir='/home/ec2-user/SageMaker/detectron2-sagemaker',\n",
    "                                   output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "                                   metric_definitions = metric_definitions,\n",
    "                                   hyperparameters = hyperparameters, \n",
    "                                   sagemaker_session=sess,\n",
    "                                   train_use_spot_instances=train_use_spot_instances,\n",
    "                                   train_max_run=train_max_run,\n",
    "                                   train_max_wait=train_max_wait,\n",
    "                                   checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "                                   profiler_config=profiler_config,\n",
    "                                    subnets=subnets,\n",
    "                                    security_group_ids=security_group_ids)\n",
    "\n",
    "\n",
    "job_name=f'd2-efstraining-spotckpt-recovery2-{int(time.time())}'\n",
    "print(f\"Launching Training Job: {job_name}\")\n",
    "data_channels = {'train': train}\n",
    "\n",
    "\n",
    "# set wait=True below if you want to print logs in cell output\n",
    "d2_efs.fit(inputs=data_channels, job_name=job_name, logs=\"All\", wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch via CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: launch_coco_train_boto3.py run-d2-sm [-h] [--bucket BUCKET]\n",
      "                                            [--image_name IMAGE_NAME]\n",
      "                                            [--metric_path METRIC_PATH]\n",
      "                                            [--job_name JOB_NAME]\n",
      "                                            [--region REGION]\n",
      "                                            [--prefix_input PREFIX_INPUT]\n",
      "                                            [--prefix_output PREFIX_OUTPUT]\n",
      "                                            [--instance_count INSTANCE_COUNT]\n",
      "                                            [--data_prefix DATA_PREFIX]\n",
      "                                            [--instance_type INSTANCE_TYPE]\n",
      "                                            [--volume_size VOLUME_SIZE]\n",
      "                                            [--use_spot] [--role ROLE]\n",
      "                                            [--max_run_time MAX_RUN_TIME]\n",
      "                                            [--max_wait_time MAX_WAIT_TIME]\n",
      "                                            [--d2_config D2_CONFIG]\n",
      "                                            [--hyperparam_path HYPERPARAM_PATH]\n",
      "\n",
      "    Utility for launching detectron2 training jobs using boto3 create_training_job API.\n",
      "    Has options for launching jobs using spot instances, if launching spot,\n",
      "    please set max_wait_time variable. Check different methods for additional documentation\n",
      "    \n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --bucket BUCKET       s3 bucket for data retrieval and storage of results\n",
      "                        (default: 'sagemaker-us-east-1-209419068016')\n",
      "  --image_name IMAGE_NAME\n",
      "                        Name of the Docker image to be used for training\n",
      "                        (default: -)\n",
      "  --metric_path METRIC_PATH\n",
      "                        Location for metric definition file (default: -)\n",
      "  --job_name JOB_NAME   Name of the SageMaker training job to launch (default:\n",
      "                        'd2-coco-train')\n",
      "  --region REGION\n",
      "  --prefix_input PREFIX_INPUT\n",
      "  --prefix_output PREFIX_OUTPUT\n",
      "  --instance_count INSTANCE_COUNT\n",
      "                        Number of instances to train on (default: 2)\n",
      "  --data_prefix DATA_PREFIX\n",
      "                        location in s3 for your data (default: 'train-coco')\n",
      "  --instance_type INSTANCE_TYPE\n",
      "                        Type of EC2 instances to train on, for (default:\n",
      "                        'ml.p3.16xlarge')\n",
      "  --volume_size VOLUME_SIZE\n",
      "                        Size of EBS volume attached to instance (default: 100)\n",
      "  --use_spot            Whether to use spot instances for training (default:\n",
      "                        False)\n",
      "  --role ROLE           SageMaker execution role (default: -)\n",
      "  --max_run_time MAX_RUN_TIME\n",
      "  --max_wait_time MAX_WAIT_TIME\n",
      "  --d2_config D2_CONFIG\n",
      "                        Detectron2 configuration file to use (default: 'COCO-\n",
      "                        InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml')\n",
      "  --hyperparam_path HYPERPARAM_PATH\n",
      "                        Location for hyperparameters file (default: -)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} launch_coco_train_boto3.py run-d2-sm --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job launched!\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} launch_coco_train_boto3.py run-d2-sm --bucket privisaa-bucket-2 --job_name d2-cli-job2 --region us-east-1 --metric_path metric_defs --hyperparam_path hyperparams.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ARN:  arn:aws:sagemaker:us-east-1:209419068016:training-job/d2-cli-job2\n",
      "Job Status:  InProgress\n",
      "Hyperparameters:  {'config-file': 'COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml', 'eval-only': 'False', 'opts': 'SOLVER.MAX_ITER 1000', 'resume': 'True'}\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} launch_coco_train_boto3.py check-d2-sm --job_name d2-cli-job2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Spot Instance with s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use_spot_instances = True\n",
    "train_max_run=21600\n",
    "train_max_wait = 30000 if train_use_spot_instances else None\n",
    "\n",
    "import uuid\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_s3_uri = 's3://{}/artifacts/d2-checkpoint-{}/'.format(bucket, checkpoint_suffix) if train_use_spot_instances else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating training-job with name: 2-nodes-max-iter-2000-genest-prof-spot6\n"
     ]
    }
   ],
   "source": [
    "container = \"d2-sm-coco-custom\" # your container name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/d2-sm-coco:distributed'.format(account, region)\n",
    "\n",
    "hyperparameters = {\"config-file\":\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\", \n",
    "                   \"resume\":\"True\", # whether to re-use weights from pre-trained model\n",
    "                   \"eval-only\":\"False\", # whether to perform only D2 model evaluation\n",
    "                   \"opts\": \"SOLVER.MAX_ITER 1000\" #  MODEL.WEIGHTS \n",
    "                   }\n",
    "\n",
    "\n",
    "d2 = sagemaker.estimator.Estimator(image,\n",
    "                                   role=role,\n",
    "                                   train_instance_count=2, \n",
    "                                   train_instance_type='ml.p3.2xlarge',\n",
    "                                   train_volume_size=100,\n",
    "                                   output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "                                   metric_definitions = metric_definitions,\n",
    "                                   hyperparameters = hyperparameters, \n",
    "                                   sagemaker_session=sess,\n",
    "                                   train_use_spot_instances=train_use_spot_instances,\n",
    "                                   train_max_run=train_max_run,\n",
    "                                   train_max_wait=train_max_wait,\n",
    "                                   checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "                                   profiler_config=profiler_config\n",
    "                                  )\n",
    "\n",
    "d2.fit({'training':f\"s3://{bucket}/train-coco\"},\n",
    "       job_name = \"2-nodes-max-iter-2000-genest-prof-spot6\",\n",
    "       wait=False,\n",
    "              experiment_config={\n",
    "            \"TrialName\": d2_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>SageMaker.ImageUri</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>config-file</th>\n",
       "      <th>eval-only</th>\n",
       "      <th>opts</th>\n",
       "      <th>resume</th>\n",
       "      <th>sagemaker_container_log_level</th>\n",
       "      <th>sagemaker_enable_cloudwatch_metrics</th>\n",
       "      <th>sagemaker_job_name</th>\n",
       "      <th>sagemaker_program</th>\n",
       "      <th>sagemaker_region</th>\n",
       "      <th>sagemaker_submit_directory</th>\n",
       "      <th>spot_ckpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-nodes-max-iter-1000-ptest-newfpath-resnestno...</td>\n",
       "      <td>Training</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:209419068016:train...</td>\n",
       "      <td>209419068016.dkr.ecr.us-east-1.amazonaws.com/d...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ml.p3.16xlarge</td>\n",
       "      <td>100.0</td>\n",
       "      <td>\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_...</td>\n",
       "      <td>\"False\"</td>\n",
       "      <td>\"SOLVER.MAX_ITER 1000\"</td>\n",
       "      <td>\"True\"</td>\n",
       "      <td>20.0</td>\n",
       "      <td>false</td>\n",
       "      <td>\"2-nodes-max-iter-1000-ptest-newfpath-resnestn...</td>\n",
       "      <td>\"/home/ec2-user/SageMaker/detectron2-sagemaker...</td>\n",
       "      <td>\"us-east-1\"</td>\n",
       "      <td>\"s3://privisaa-bucket-2/2-nodes-max-iter-1000-...</td>\n",
       "      <td>\"\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName DisplayName  \\\n",
       "0  2-nodes-max-iter-1000-ptest-newfpath-resnestno...    Training   \n",
       "\n",
       "                                           SourceArn  \\\n",
       "0  arn:aws:sagemaker:us-east-1:209419068016:train...   \n",
       "\n",
       "                                  SageMaker.ImageUri  SageMaker.InstanceCount  \\\n",
       "0  209419068016.dkr.ecr.us-east-1.amazonaws.com/d...                      2.0   \n",
       "\n",
       "  SageMaker.InstanceType  SageMaker.VolumeSizeInGB  \\\n",
       "0         ml.p3.16xlarge                     100.0   \n",
       "\n",
       "                                         config-file eval-only  \\\n",
       "0  \"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_...   \"False\"   \n",
       "\n",
       "                     opts  resume  sagemaker_container_log_level  \\\n",
       "0  \"SOLVER.MAX_ITER 1000\"  \"True\"                           20.0   \n",
       "\n",
       "  sagemaker_enable_cloudwatch_metrics  \\\n",
       "0                               false   \n",
       "\n",
       "                                  sagemaker_job_name  \\\n",
       "0  \"2-nodes-max-iter-1000-ptest-newfpath-resnestn...   \n",
       "\n",
       "                                   sagemaker_program sagemaker_region  \\\n",
       "0  \"/home/ec2-user/SageMaker/detectron2-sagemaker...      \"us-east-1\"   \n",
       "\n",
       "                          sagemaker_submit_directory spot_ckpt  \n",
       "0  \"s3://privisaa-bucket-2/2-nodes-max-iter-1000-...        \"\"  "
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_expression = {\n",
    "    \"Filters\":[\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=Session(b3sess, sm), \n",
    "    experiment_name=d2_experiment.experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.test:accuracy.max\",\n",
    "    sort_order=\"Descending\",\n",
    "    metric_names=['test:accuracy'],\n",
    ")\n",
    "\n",
    "trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TrainingJobName': 'd2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318',\n",
       " 'TrainingJobArn': 'arn:aws:sagemaker:us-east-1:209419068016:training-job/d2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318',\n",
       " 'ModelArtifacts': {'S3ModelArtifacts': 's3://privisaa-bucket-2/detectron2-output/d2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318/output/model.tar.gz'},\n",
       " 'TrainingJobStatus': 'Completed',\n",
       " 'SecondaryStatus': 'Completed',\n",
       " 'HyperParameters': {'config-file': '\"COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml\"',\n",
       "  'eval-only': '\"False\"',\n",
       "  'opts': '\"SOLVER.MAX_ITER 1000\"',\n",
       "  'resume': '\"True\"',\n",
       "  'sagemaker_container_log_level': '20',\n",
       "  'sagemaker_enable_cloudwatch_metrics': 'false',\n",
       "  'sagemaker_job_name': '\"d2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318\"',\n",
       "  'sagemaker_program': '\"/home/ec2-user/SageMaker/detectron2-sagemaker/container_training/train_coco_efs.py\"',\n",
       "  'sagemaker_region': '\"us-east-1\"',\n",
       "  'sagemaker_submit_directory': '\"s3://privisaa-bucket-2/d2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318/source/sourcedir.tar.gz\"',\n",
       "  'spot_ckpt': '\"\"'},\n",
       " 'AlgorithmSpecification': {'TrainingImage': '209419068016.dkr.ecr.us-east-1.amazonaws.com/d2-sm-coco2:distributed',\n",
       "  'TrainingInputMode': 'File',\n",
       "  'MetricDefinitions': [{'Name': 'total_loss',\n",
       "    'Regex': '.*total_loss:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'loss_cls', 'Regex': '.*loss_cls:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'loss_box_reg', 'Regex': '.*loss_box_reg:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'loss_mask', 'Regex': '.*loss_mask:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'loss_rpn_cls', 'Regex': '.*loss_rpn_cls:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'loss_rpn_loc', 'Regex': '.*loss_rpn_loc:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'overall_training_speed',\n",
       "    'Regex': '.*Overall training speed:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'lr', 'Regex': '.*lr:\\\\s([0-9\\\\.]+)\\\\s*'},\n",
       "   {'Name': 'iter', 'Regex': '.*iter:\\\\s([0-9\\\\.]+)\\\\s*'}],\n",
       "  'EnableSageMakerMetricsTimeSeries': True},\n",
       " 'RoleArn': 'arn:aws:iam::209419068016:role/service-role/AmazonSageMaker-ExecutionRole-20200814T102098',\n",
       " 'InputDataConfig': [{'ChannelName': 'train',\n",
       "   'DataSource': {'FileSystemDataSource': {'FileSystemId': 'fs-d120c724',\n",
       "     'FileSystemAccessMode': 'ro',\n",
       "     'FileSystemType': 'EFS',\n",
       "     'DirectoryPath': '/training'}},\n",
       "   'CompressionType': 'None',\n",
       "   'RecordWrapperType': 'None'}],\n",
       " 'OutputDataConfig': {'KmsKeyId': '',\n",
       "  'S3OutputPath': 's3://privisaa-bucket-2/detectron2-output'},\n",
       " 'ResourceConfig': {'InstanceType': 'ml.p3.2xlarge',\n",
       "  'InstanceCount': 2,\n",
       "  'VolumeSizeInGB': 100},\n",
       " 'VpcConfig': {'SecurityGroupIds': ['sg-317ad11a'],\n",
       "  'Subnets': ['subnet-9e445ef9']},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 10000,\n",
       "  'MaxWaitTimeInSeconds': 10000},\n",
       " 'CreationTime': datetime.datetime(2020, 10, 21, 15, 15, 42, 242000, tzinfo=tzlocal()),\n",
       " 'TrainingStartTime': datetime.datetime(2020, 10, 21, 15, 18, 27, 595000, tzinfo=tzlocal()),\n",
       " 'TrainingEndTime': datetime.datetime(2020, 10, 21, 15, 38, 14, 933000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2020, 10, 21, 15, 38, 14, 933000, tzinfo=tzlocal()),\n",
       " 'SecondaryStatusTransitions': [{'Status': 'Starting',\n",
       "   'StartTime': datetime.datetime(2020, 10, 21, 15, 15, 42, 242000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 10, 21, 15, 18, 27, 595000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Preparing the instances for training'},\n",
       "  {'Status': 'Downloading',\n",
       "   'StartTime': datetime.datetime(2020, 10, 21, 15, 18, 27, 595000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 10, 21, 15, 18, 36, 514000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Downloading input data'},\n",
       "  {'Status': 'Training',\n",
       "   'StartTime': datetime.datetime(2020, 10, 21, 15, 18, 36, 514000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 10, 21, 15, 36, 36, 665000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Training image download completed. Training in progress.'},\n",
       "  {'Status': 'Uploading',\n",
       "   'StartTime': datetime.datetime(2020, 10, 21, 15, 36, 36, 665000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 10, 21, 15, 38, 14, 933000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Uploading generated training model'},\n",
       "  {'Status': 'Completed',\n",
       "   'StartTime': datetime.datetime(2020, 10, 21, 15, 38, 14, 933000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 10, 21, 15, 38, 14, 933000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Training job completed'}],\n",
       " 'FinalMetricDataList': [{'MetricName': 'total_loss',\n",
       "   'Value': 1.7690000534057617,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'loss_cls',\n",
       "   'Value': 0.4020000100135803,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'loss_box_reg',\n",
       "   'Value': 0.1979999989271164,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'loss_mask',\n",
       "   'Value': 0.5450000166893005,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'loss_rpn_cls',\n",
       "   'Value': 0.3700000047683716,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'loss_rpn_loc',\n",
       "   'Value': 0.25699999928474426,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'lr',\n",
       "   'Value': 0.01998000033199787,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'iter',\n",
       "   'Value': 1000.0,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 13, 21, 34, 271000, tzinfo=tzlocal())}],\n",
       " 'EnableNetworkIsolation': False,\n",
       " 'EnableInterContainerTrafficEncryption': False,\n",
       " 'EnableManagedSpotTraining': True,\n",
       " 'CheckpointConfig': {'S3Uri': 's3://privisaa-bucket-2/artifacts/d2-efs-checkpoint-57a35863/'},\n",
       " 'TrainingTimeInSeconds': 1187,\n",
       " 'BillableTimeInSeconds': 356,\n",
       " 'DebugHookConfig': {'S3OutputPath': 's3://privisaa-bucket-2/detectron2-output',\n",
       "  'CollectionConfigurations': []},\n",
       " 'ProfilerConfig': {'S3OutputPath': 's3://privisaa-bucket-2/detectron2-ouput',\n",
       "  'ProfilingIntervalInMilliseconds': 500,\n",
       "  'ProfilingParameters': {'GeneralMetricsConfig': '{\"StartStep\": \"2\", \"NumSteps\": \"2\"}',\n",
       "   'ProfilerEnabled': 'True'}},\n",
       " 'ResponseMetadata': {'RequestId': 'c8026794-d5de-4669-b813-06a4a0425e43',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c8026794-d5de-4669-b813-06a4a0425e43',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '4821',\n",
       "   'date': 'Wed, 21 Oct 2020 16:39:50 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.describe_training_job(TrainingJobName='d2-efs-efstraining-prevd2-spotfix-spotckptquotes-1603293318')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiler Setup\n",
    "\n",
    "In order to use SageMaker Debugger's Profiler we need to specify a configuration we will hook into our estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup profiler hooks\n",
    "\n",
    "from sagemaker.profiler import ProfilerConfig \n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    profiling_interval_millis=500,\n",
    "    profiling_parameters={\n",
    "        \"ProfilerEnabled\": str(True),\n",
    "        \"GeneralMetricsConfig\": \"{\\\"StartStep\\\": \\\"2\\\", \\\"NumSteps\\\": \\\"2\\\"}\"\n",
    "   }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
